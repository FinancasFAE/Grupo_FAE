[
  {
    "objectID": "terceiro_encontro.html",
    "href": "terceiro_encontro.html",
    "title": "Projeto Finanças",
    "section": "",
    "text": "!pip install yahooquery\n\nCollecting yahooquery\n  Downloading yahooquery-2.3.7-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: beautifulsoup4&lt;5.0.0,&gt;=4.12.2 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (4.13.3)\nCollecting lxml&lt;5.0.0,&gt;=4.9.3 (from yahooquery)\n  Downloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: pandas&lt;3.0.0,&gt;=2.0.3 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (2.2.2)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.31.0 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (2.32.3)\nCollecting requests-futures&lt;2.0.0,&gt;=1.0.1 (from yahooquery)\n  Downloading requests_futures-1.0.2-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.65.0 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (4.67.1)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.2-&gt;yahooquery) (2.6)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.2-&gt;yahooquery) (4.12.2)\nRequirement already satisfied: numpy&gt;=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2.0.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2025.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2025.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (2025.1.31)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (1.17.0)\nDownloading yahooquery-2.3.7-py3-none-any.whl (52 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 kB 2.0 MB/s eta 0:00:00\nDownloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl (7.9 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 49.3 MB/s eta 0:00:00\nDownloading requests_futures-1.0.2-py2.py3-none-any.whl (7.7 kB)\nInstalling collected packages: lxml, requests-futures, yahooquery\n  Attempting uninstall: lxml\n    Found existing installation: lxml 5.3.1\n    Uninstalling lxml-5.3.1:\n      Successfully uninstalled lxml-5.3.1\nSuccessfully installed lxml-4.9.4 requests-futures-1.0.2 yahooquery-2.3.7\n\n\n\n!pip install arch\n\nCollecting arch\n  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy&gt;=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (2.0.2)\nRequirement already satisfied: scipy&gt;=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.14.1)\nRequirement already satisfied: pandas&gt;=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.2)\nRequirement already satisfied: statsmodels&gt;=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2025.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2025.1)\nRequirement already satisfied: patsy&gt;=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels&gt;=0.12-&gt;arch) (1.0.1)\nRequirement already satisfied: packaging&gt;=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels&gt;=0.12-&gt;arch) (24.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.4-&gt;arch) (1.17.0)\nDownloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 985.3/985.3 kB 8.9 MB/s eta 0:00:00\nInstalling collected packages: arch\nSuccessfully installed arch-7.2.0\n\n\n\n#import yfinance as yf\nfrom yahooquery import Ticker\nimport pandas as pd\nimport numpy as np\n#import matplotlib.pyplot as plt\n#import seaborn as sns\nfrom datetime import datetime\nfrom arch import arch_model # Lib do Python pra estimar as volatilidades (ARCH/GARCH)\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal\n\n\n# Tickers for portfolio\nTICKERS = [\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n]\n\n# Baixar os dados históricos com yahooquery\ntickers = Ticker(TICKERS)\ndata = tickers.history(period=\"5y\")\n\n# Resetar o índice corretamente\ndata = data.reset_index()\n\n# O yahooquery retorna um MultiIndex, então é preciso garantir que a coluna \"date\" exista corretamente\nif \"date\" not in data.columns:\n    raise ValueError(\"A coluna 'date' não foi encontrada no dataset! Verifique a estrutura do DataFrame.\")\n\n# Selecionar apenas as colunas de interesse e reformatar\nportfolio_prices = data.pivot(index=\"date\", columns=\"symbol\", values=\"close\").reset_index()\n\n# Garantir que não há valores ausentes\nportfolio_prices.dropna(inplace=True)\n\nportfolio_prices.head()\n\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/ticker.py:1333: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n\n\n  \n    \n\n\n\n\n\nsymbol\ndate\nBEEF3.SA\nBRFS3.SA\nGIS\nHRL\nJBSS3.SA\nMRFG3.SA\nTSN\n\n\n\n\n0\n2020-03-23\n8.20\n13.355227\n47.279999\n42.310001\n19.340000\n7.387873\n57.599998\n\n\n1\n2020-03-24\n8.40\n14.316651\n48.230000\n44.849998\n21.219999\n7.855208\n59.990002\n\n\n2\n2020-03-25\n8.19\n15.144808\n47.650002\n41.939999\n22.049999\n8.242997\n63.189999\n\n\n3\n2020-03-26\n7.70\n15.201922\n50.000000\n44.849998\n21.870001\n8.352373\n61.230000\n\n\n4\n2020-03-27\n7.80\n14.154827\n51.820000\n44.959999\n20.889999\n8.471693\n58.590000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\nimport plotly.express as px\n\n# Certifique-se de que a coluna \"date\" está em formato datetime\nportfolio_prices['date'] = pd.to_datetime(portfolio_prices['date'])\n\n# Transformar os preços para o formato longo (melt) para facilitar o plot\nprices_long = portfolio_prices.melt(id_vars='date', var_name='Ativo', value_name='Valor')\n\nfig = px.line(prices_long, x='date', y='Valor', color='Ativo',\n              title='Séries Temporais de Preços')\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n# Calcular os log-retornos\nlog_returns = portfolio_prices.copy()\nlog_returns.iloc[:, 1:] = np.log(portfolio_prices.iloc[:, 1:]).diff()\n\n# Remover a primeira linha que contém NaN após a diferenciação\nlog_returns = log_returns.dropna()\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\n# Garantir que a coluna \"date\" esteja em formato datetime\nlog_returns['date'] = pd.to_datetime(log_returns['date'])\n\n# Transformar para formato longo\nlog_returns_long = log_returns.melt(id_vars='date', var_name='Ativo', value_name='Log_Retorno')\n\nfig = px.line(log_returns_long, x='date', y='Log_Retorno', color='Ativo',\n              title='Séries Temporais de Log-Retornos')\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Seleciona apenas as colunas dos ativos (excluindo a coluna \"date\")\nativos = log_returns.columns[1:]\n\n# Calcula a assimetria para cada ativo\nskewness = log_returns[ativos].skew()\n\n# Cria um DataFrame para visualizar os resultados\nskew_table = pd.DataFrame({\n    'Ativo': skewness.index,\n    'Skewness': skewness.values\n})\n\n# Adiciona a coluna que indica a direção da assimetria\nskew_table['Direcao'] = skew_table['Skewness'].apply(\n    lambda x: 'À direita' if x &gt; 0 else ('À esquerda' if x &lt; 0 else 'Simétrica')\n)\n\n# Exibe a tabela atualizada\nskew_table\n\n\n  \n    \n\n\n\n\n\n\nAtivo\nSkewness\nDirecao\n\n\n\n\n0\nBEEF3.SA\n-0.322431\nÀ esquerda\n\n\n1\nBRFS3.SA\n0.334181\nÀ direita\n\n\n2\nGIS\n-0.307135\nÀ esquerda\n\n\n3\nHRL\n-0.358598\nÀ esquerda\n\n\n4\nJBSS3.SA\n0.526435\nÀ direita\n\n\n5\nMRFG3.SA\n-0.079424\nÀ esquerda\n\n\n6\nTSN\n-0.945498\nÀ esquerda\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Retire o comando #| eval: false caso queira executar essa celula\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Transformar os log-retornos para formato longo (melt)\nlog_returns_long = log_returns.melt(id_vars=[\"date\"], var_name=\"Ativo\", value_name=\"Log_Retorno\")\n\n# Criar gráfico com Seaborn\nplt.figure(figsize=(12, 8))\ng = sns.FacetGrid(log_returns_long, col=\"Ativo\", col_wrap=2, sharex=False, sharey=False)\ng.map_dataframe(sns.histplot, x=\"Log_Retorno\", kde=True, bins=30, color=\"black\", alpha=0.5)\n\n# Ajustar título dos gráficos\ng.set_titles(col_template=\"{col_name}\")\n\n# Melhorar layout\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# Calcular a volatilidade histórica com uma janela móvel de 5 dias\n\n# Supondo que 'log_returns' já foi calculado e possui a coluna 'date' e os log-retornos dos ativos\nwindow = 5\n\n# Cria um DataFrame para armazenar a volatilidade histórica\nvol_hist = pd.DataFrame({'date': log_returns[\"date\"]})\n\n# Calcula o desvio-padrão móvel (volatilidade) para cada ativo\nfor col in log_returns.columns[1:]:\n    vol_hist[col] = log_returns[col].rolling(window=window).std()\n\n# Exibe as ultimas linhas do DataFrame de volatilidade histórica\n#print(vol_hist.head()) # 5 primeiros serão NaN\nprint(vol_hist.tail())\n\n           date  BEEF3.SA  BRFS3.SA       GIS       HRL  JBSS3.SA  MRFG3.SA  \\\n1238 2025-03-14  0.031093  0.024722  0.024976  0.014724  0.015667  0.024863   \n1239 2025-03-17  0.031816  0.026581  0.028208  0.015502  0.016132  0.028757   \n1240 2025-03-18  0.022213  0.040627  0.021501  0.014557  0.073009  0.036127   \n1241 2025-03-19  0.021905  0.039398  0.015938  0.007907  0.073728  0.035161   \n1242 2025-03-20  0.033103  0.034645  0.016877  0.007681  0.071088  0.039908   \n\n           TSN  \n1238  0.011254  \n1239  0.011458  \n1240  0.011033  \n1241  0.006089  \n1242  0.006956  \n\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\n# Certificar que \"date\" é datetime (já feito)\nvol_hist['date'] = pd.to_datetime(vol_hist['date'])\n\n# Transformar para formato longo\nvol_hist_long = vol_hist.melt(id_vars='date', var_name='Ativo', value_name='Volatilidade_Hist')\n\nfig = px.line(vol_hist_long, x='date', y='Volatilidade_Hist', color='Ativo',\n              title='Volatilidade Histórica (Desvio-Padrão) com janela de 5 dias')\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Estimar o modelo GARCH(1,1) e salvar variância condicional\nvar_condicional = pd.DataFrame({\"date\": log_returns[\"date\"]})\n\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n    var_condicional[col] = res.conditional_volatility ** 2\n\nvar_condicional.head()\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007185. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009805. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001789. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:768: ConvergenceWarning:\n\nThe optimizer returned code 4. The message is:\nInequality constraints incompatible\nSee scipy.optimize.fmin_slsqp for code meaning.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002188. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000497. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000827. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000305. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\n\n\n  \n    \n\n\n\n\n\n\ndate\nBEEF3.SA\nBRFS3.SA\nGIS\nHRL\nJBSS3.SA\nMRFG3.SA\nTSN\n\n\n\n\n1\n2020-03-24\n0.001141\n0.008166\n0.000270\n0.001420\n0.005637\n0.001305\n0.002915\n\n\n2\n2020-03-25\n0.000994\n0.008166\n0.000148\n0.006134\n0.011258\n0.001398\n0.003002\n\n\n3\n2020-03-26\n0.000894\n0.008166\n0.000103\n0.002925\n0.005405\n0.001291\n0.003657\n\n\n4\n2020-03-27\n0.001153\n0.008166\n0.001017\n0.007534\n0.004224\n0.001003\n0.002935\n\n\n5\n2020-03-30\n0.000958\n0.008166\n0.000540\n0.000928\n0.005940\n0.000853\n0.003649\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n# Inferir sobre os parâmetros do modelo GARCH(1,1) para cada ativo do portfólio\n\nparams_list = []\n\n# Iterar sobre cada ativo (exceto a coluna 'date')\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n\n    par = res.params\n    alpha_val = par.get(\"alpha[1]\", None)\n    beta_val  = par.get(\"beta[1]\", None)\n    alpha_beta_sum = (alpha_val if alpha_val is not None else 0) + (beta_val if beta_val is not None else 0)\n\n    # Interpretação curta\n    if alpha_beta_sum &gt;= 0.9:\n        interp = f\"Alta persistência (α+β = {alpha_beta_sum:.4f}).\"\n    else:\n        interp = f\"Baixa/moderada persistência (α+β = {alpha_beta_sum:.4f}).\"\n\n    params_list.append({\n         \"Ativo\": col,\n         \"mu\": par.get(\"mu\", None),\n         \"omega\": par.get(\"omega\", None),\n         \"alpha\": alpha_val,\n         \"beta\": beta_val,\n         \"alpha+beta\": alpha_beta_sum,\n         \"nu\": par.get(\"nu\", None),\n         \"Interpretacao\": interp\n    })\n\ngarch_params = pd.DataFrame(params_list)\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007185. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009805. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001789. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:768: ConvergenceWarning:\n\nThe optimizer returned code 4. The message is:\nInequality constraints incompatible\nSee scipy.optimize.fmin_slsqp for code meaning.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002188. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000497. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000827. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000305. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BEEF3.SA']].copy()\nvol_zc = var_condicional[['date', 'BEEF3.SA']].copy()\n\n# Converter \"date\" para datetime, se necessário\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Diários - BEEF3.SA\", \"Volatilidade Condicional (GARCH) - BEEF3.SA\")\n)\n\n# Adicionar o gráfico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BEEF3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gráfico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BEEF3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BEEF3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()"
  },
  {
    "objectID": "Projeto Finanças/about.html",
    "href": "Projeto Finanças/about.html",
    "title": "Sobre Nós",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJoão Niquele"
  },
  {
    "objectID": "Projeto Finanças/about.html#sumário",
    "href": "Projeto Finanças/about.html#sumário",
    "title": "Sobre Nós",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJoão Niquele"
  },
  {
    "objectID": "Projeto Finanças/about.html#arthur-lauffer",
    "href": "Projeto Finanças/about.html#arthur-lauffer",
    "title": "Sobre Nós",
    "section": "Arthur Lauffer",
    "text": "Arthur Lauffer\n\nCargo: É analista de BI e estudante de Ciência de Dados para Negócios na FAE Business School. Ele administra sua própria empresa de BI, prestando serviços para outras empresas, e também gerencia uma empresa de SaaS focada em projetos de longo prazo. Com grande experiência em Power BI, ele desenvolve dashboards e modelos de dados para diversas áreas, incluindo vendas, RH e faturamento. Além disso, atua como administrador do Workspace do Google da sua empresa. No tempo livre, tem interesse em música eletrônica e está organizando a festa Synapse. 🔗 Portfolio"
  },
  {
    "objectID": "Projeto Finanças/about.html#davi-kemper",
    "href": "Projeto Finanças/about.html#davi-kemper",
    "title": "Sobre Nós",
    "section": "Daniel K Junior",
    "text": "Daniel K Junior\n\nCargo: Formado na Escola de Sargento das Armas no ano de 2021, decidiu fazer a transição de carreira para a área de Dados já no ínicio da faculdade, concluindo a transição no final do ano de 2024, hoje atua como Analista de BI na EZ Chart.\n🔗 Portfolio"
  },
  {
    "objectID": "Projeto Finanças/about.html#davi-kemper-1",
    "href": "Projeto Finanças/about.html#davi-kemper-1",
    "title": "Sobre Nós",
    "section": "Davi Kemper",
    "text": "Davi Kemper\n\nCargo: Estudante de Ciência de Dados na FAE, atuou como Analista de BI do grupo Metronorte. 🔗 Portfolio"
  },
  {
    "objectID": "Projeto Finanças/about.html#joão-niquele",
    "href": "Projeto Finanças/about.html#joão-niquele",
    "title": "Sobre Nós",
    "section": "João Niquele",
    "text": "João Niquele\n\nCargo: Estudante de Ciência de Dados na FAE 🔗 Portfolio"
  },
  {
    "objectID": "FeatEngFinancial.html#introdução-feature-engineering-em-dados-de-séries-financeiras",
    "href": "FeatEngFinancial.html#introdução-feature-engineering-em-dados-de-séries-financeiras",
    "title": "Feature Engineering com séries de preços de ativos financeiros",
    "section": "📌 Introdução: Feature Engineering em dados de séries financeiras",
    "text": "📌 Introdução: Feature Engineering em dados de séries financeiras\n\n\nMedindo a Volatilidade: Com e Sem GARCH\nA abordagem tradicional calcula a volatilidade como o desvio padrão dos retornos históricos em uma janela móvel de tamanho \\(N\\): (dias de negociação)\n\\[\n\\sigma_{\\text{hist}} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(r_i - \\bar{r})^2}\n\\]\nVantagens:\n\nSimplicidade e facilidade de implementação.\n\nDesvantagens:\n\nAssume volatilidade constante durante a janela. (ou seja precisa de um range dias e não é capaz de medir o desvio ou o risco/volatilidade de um dia pro outro.)\nNão capta a persistência dos choques (efeito de clustering).\nNão reage dinamicamente a choques recentes.\n\n\n\nVolatilidade com o GARCH\nO modelo GARCH(1,1) estima a variância condicional de forma dinâmica:\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2.\n\\]\nOnde: - \\(\\epsilon_{t-1}^2\\) reflete o impacto dos choques recentes. - \\(\\sigma_{t-1}^2\\) reflete a persistência da volatilidade do período anterior. - \\(\\omega &gt; 0\\), \\(\\alpha \\geq 0\\) e \\(\\beta \\geq 0\\) são parâmetros estimados.\nA soma \\(\\alpha + \\beta\\) mede a persistência total da volatilidade:\n\nPróximo de 1: Choques têm efeitos duradouros; a volatilidade permanece alta por vários períodos.\nMenor que 1: Os choques se dissipam mais rapidamente; a volatilidade retorna ao seu nível médio mais rápido.\n\nVantagens do GARCH:\n\nModela a volatilidade de forma dinâmica.\nCaptura o efeito de “clustering” dos choques.\nPermite previsões mais precisas da volatilidade futura.\n\nDesvantagens do GARCH:\n\nRequer estimação de parâmetros e pressupõe uma estrutura específica para a volatilidade.\nPode ser sensível à escolha da distribuição dos resíduos (por exemplo, normal vs. \\(t\\) de Student).\n\nVolatilidade com desvio-padrãoVolatilidade com GARCH(1,1)Plotando retorno x risco\n\n\nA volatilidade histórica pode ser medida como o desvio-padrão dos log-retornos calculado em uma janela móvel de \\(N\\) dias de negociação.\nNeste exemplo, adotamos uma janela móvel de 5 dias (aproximadamente uma semana de negociação. Para um mês de negociação, utilizar 22 e um ano 252) para capturar a volatilidade diária dos ativos.\nVantagens:\n\nSimplicidade e facilidade de implementação.\n\nDesvantagens:\n\nAssume volatilidade constante durante a janela.\nNão capta a persistência dos choques.\nNão reage dinamicamente a choques recentes.\n\n\n\nCode\n# Calcular a volatilidade histórica com uma janela móvel de 5 dias\n\n# Supondo que 'log_returns' já foi calculado e possui a coluna 'date' e os log-retornos dos ativos\nwindow = 5\n\n# Cria um DataFrame para armazenar a volatilidade histórica\nvol_hist = pd.DataFrame({'date': log_returns[\"date\"]})\n\n# Calcula o desvio-padrão móvel (volatilidade) para cada ativo\nfor col in log_returns.columns[1:]:\n    vol_hist[col] = log_returns[col].rolling(window=window).std()\n\n# Exibe as ultimas linhas do DataFrame de volatilidade histórica\n#print(vol_hist.head()) # 5 primeiros serão NaN\nprint(vol_hist.tail())\n\n\n            date  BEEF3.SA  BRFS3.SA       GIS       HRL  JBSS3.SA  MRFG3.SA  \\\n1239  2025-04-04  0.018061  0.017195  0.019901  0.019937  0.013475  0.017031   \n1240  2025-04-07  0.017915  0.017151  0.020406  0.020119  0.012847  0.017031   \n1241  2025-04-08  0.016138  0.007259  0.024098  0.021751  0.008219  0.004705   \n1242  2025-04-09  0.042623  0.038677  0.028590  0.024068  0.023526  0.025228   \n1243  2025-04-10  0.040566  0.038906  0.021658  0.016153  0.022935  0.023309   \n\n           TSN  \n1239  0.032352  \n1240  0.027654  \n1241  0.027724  \n1242  0.037064  \n1243  0.036158  \n\n\nPlotando temos:\n\n\nCode\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\n# Certificar que \"date\" é datetime (já feito)\nvol_hist['date'] = pd.to_datetime(vol_hist['date'])\n\n# Transformar para formato longo\nvol_hist_long = vol_hist.melt(id_vars='date', var_name='Ativo', value_name='Volatilidade_Hist')\n\nfig = px.line(vol_hist_long, x='date', y='Volatilidade_Hist', color='Ativo',\n              title='Volatilidade Histórica (Desvio-Padrão) com janela de 5 dias')\nfig.show()\n\n\n\n\nO desvio-padrão assume que precisaremos de um range maior do que 2 pontos no tempo, o que limita nossa análise pois a incompatibiliza, uma vez que precisaremos comparar os riscos diários x retornos diários (como feito anteriormente). Ou seja, não podemos comparar retornos dia-a-dia x volatilidades (risco) de 5 em 5 dias p. ex.\nOs modelos heterocedásticos (da família ARCH) estimam a variância condicional dos nossos dados, ou seja, em linguagem de finanças, eles são capazes de capturar as volatilidades ou risco dos retornos dos preços de ativos financeiros ponto a ponto no tempo, ou seja, dia a dia.\nO modelo GARCH(1,1) com distribuição \\(t\\) assimétrica não está disponível diretamente na maioria das bibliotecas Python. No entanto, podemos utilizar um GARCH(1,1) com uma distribuição \\(t\\) padrão para estimar a variância condicional. O modelo é representado por:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nOnde:\n\n\\(r_t\\) é o log-retorno no tempo \\(t\\).\n\\(\\mu\\) é a média dos retornos.\n\\(\\epsilon_t\\) é o termo de erro, condicionado às informações passadas.\n\\(\\sigma_t^2\\) é a variância condicional no tempo \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) são os parâmetros a serem estimados, com \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) segue uma distribuição \\(t\\) de Student com \\(ν\\) graus de liberdade para capturar as caudas pesadas observadas em retornos financeiros.\n\nA soma \\(\\alpha + \\beta\\) é frequentemente utilizada para medir a persistência da volatilidade: quanto mais próximos de 1, maior a persistência dos choques na volatilidade.\nVamos estimar a variância condicional (\\(\\sigma^2_{t}\\) ) para cada ativo:\n\n\nCode\n# Estimar o modelo GARCH(1,1) e salvar variância condicional\nvar_condicional = pd.DataFrame({\"date\": log_returns[\"date\"]})\n\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n    var_condicional[col] = res.conditional_volatility ** 2\n\nvar_condicional.head()\n\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007186. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009502. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001738. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002101. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0004777. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0008154. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002983. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\n\n\n\n\n\n\n\ndate\nBEEF3.SA\nBRFS3.SA\nGIS\nHRL\nJBSS3.SA\nMRFG3.SA\nTSN\n\n\n\n\n1\n2020-04-14\n0.006489\n0.003739\n0.000189\n0.002342\n0.000879\n0.001129\n0.002966\n\n\n2\n2020-04-15\n0.005795\n0.003823\n0.000202\n0.002844\n0.000781\n0.000866\n0.002396\n\n\n3\n2020-04-16\n0.005688\n0.003715\n0.000179\n0.002033\n0.000779\n0.000740\n0.002656\n\n\n4\n2020-04-17\n0.007190\n0.003715\n0.000222\n0.002954\n0.000672\n0.000705\n0.002138\n\n\n5\n2020-04-20\n0.006506\n0.003712\n0.000190\n0.002054\n0.000642\n0.000673\n0.002299\n\n\n\n\n\n\n\nVamos avaliar os parâmetros estimados do modelo:\n\n\nCode\n# Inferir sobre os parâmetros do modelo GARCH(1,1) para cada ativo do portfólio\n\nparams_list = []\n\n# Iterar sobre cada ativo (exceto a coluna 'date')\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n\n    par = res.params\n    alpha_val = par.get(\"alpha[1]\", None)\n    beta_val  = par.get(\"beta[1]\", None)\n    alpha_beta_sum = (alpha_val if alpha_val is not None else 0) + (beta_val if beta_val is not None else 0)\n\n    # Interpretação curta\n    if alpha_beta_sum &gt;= 0.9:\n        interp = f\"Alta persistência (α+β = {alpha_beta_sum:.4f}).\"\n    else:\n        interp = f\"Baixa/moderada persistência (α+β = {alpha_beta_sum:.4f}).\"\n\n    params_list.append({\n         \"Ativo\": col,\n         \"mu\": par.get(\"mu\", None),\n         \"omega\": par.get(\"omega\", None),\n         \"alpha\": alpha_val,\n         \"beta\": beta_val,\n         \"alpha+beta\": alpha_beta_sum,\n         \"nu\": par.get(\"nu\", None),\n         \"Interpretacao\": interp\n    })\n\ngarch_params = pd.DataFrame(params_list)\n\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007186. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009502. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001738. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002101. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0004777. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0008154. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002983. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\nA soma \\(\\alpha + \\beta\\) é um indicador crucial na modelagem GARCH para avaliar a persistência da volatilidade. Em termos práticos, os parâmetros \\(\\alpha\\) e \\(\\beta\\) têm funções distintas:\n\n\\(\\alpha\\): Representa o impacto dos choques recentes (a inovação ou termo de erro \\(\\epsilon_{t-1}^2\\) na volatilidade atual. Um valor mais alto de \\(\\alpha\\) indica que choques recentes têm um efeito maior em aumentar a volatilidade.\n\\(\\beta\\): Captura a persistência da volatilidade ao longo do tempo, ou seja, o efeito da volatilidade passada (\\(\\sigma_{t-1}^2)\\) sobre a volatilidade presente. Valores maiores de \\(\\beta\\) sugerem que a volatilidade tende a se manter elevada por um período mais longo.\n\nQuando somamos esses dois parâmetros, ou seja, quando calculamos \\(\\alpha + \\beta\\), obtemos uma medida da persistência total da volatilidade:\n\nSe \\(\\alpha + \\beta\\) estiver próximo de 1, isso indica que os choques que afetam a volatilidade têm efeitos de longa duração. Em outras palavras, um choque na volatilidade tem um impacto que se dissipa muito lentamente, mantendo a volatilidade elevada por vários períodos.\nSe \\(\\alpha + \\beta\\) for significativamente menor que 1, os efeitos dos choques são de curta duração e a volatilidade retorna rapidamente ao seu nível médio após um impacto.\n\nEm alguns casos, quando \\(\\alpha + \\beta = 1\\), o modelo é denominado IGARCH (Integrated GARCH), o que implica que os choques têm efeitos persistentes permanentemente, ou seja, a volatilidade não reverte para um valor médio fixo.\nEsta característica é particularmente importante na análise de séries financeiras, pois a persistência alta da volatilidade pode implicar maior risco de mercado e desafios na previsão dos retornos futuros. Assim, a soma \\(\\alpha + \\beta\\) serve como uma medida de “memória” dos choques, indicando se a volatilidade reage de forma passageira ou duradoura a eventos inesperados.\nGraficamente temos:\n\n\nCode\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BRFS3.SA']].copy()\nvol_zc = var_condicional[['date', 'BRFS3.SA']].copy()\n\n# Converter \"date\" para datetime, se necessário\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Diários - BRFS3.SA\", \"Volatilidade Condicional (GARCH) - BRFS3.SA\")\n)\n\n# Adicionar o gráfico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BRFS3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gráfico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BRFS3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BRFS3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()\n\n\nEm alguns casos, a variância condicional pode apresentar grandes oscilações se houver outliers nos retornos ou problemas de convergência do modelo. Verifique:\n\nQualidade e limpeza dos dados\nResumo do ajuste (parâmetros \\(\\alpha,\\beta\\) plausíveis?)\nDistribuição (\\(t\\) vs. normal)\nModelos alternativos (EGARCH, GJR-GARCH, etc.)\n\n\n\nAqui iremos visualizar o comportamento do ativo ZC=F, futuros de milho:\n\n\nCode\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BRFS3.SA']].copy()\nvol_zc = var_condicional[['date', 'BRFS3.SA']].copy()\n\n# Converter \"date\" para datetime, se necessário\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Diários - BRFS3.SA\", \"Volatilidade Condicional (GARCH) - BRFS3.SA\")\n)\n\n# Adicionar o gráfico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BRFS3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gráfico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BRFS3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BRFS3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()\n\n\nNotem como o GARCH(1,1) captura bem os picos/quebras e na volatilidade dos retornos."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre Nós",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJoão Niquele"
  },
  {
    "objectID": "about.html#sumário",
    "href": "about.html#sumário",
    "title": "Sobre Nós",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJoão Niquele"
  },
  {
    "objectID": "about.html#arthur-lauffer",
    "href": "about.html#arthur-lauffer",
    "title": "Sobre Nós",
    "section": "Arthur Lauffer",
    "text": "Arthur Lauffer\n\nCargo: É analista de BI e estudante de Ciência de Dados para Negócios na FAE Business School. Ele administra sua própria empresa de BI, prestando serviços para outras empresas, e também gerencia uma empresa de SaaS focada em projetos de longo prazo. Com grande experiência em Power BI, ele desenvolve dashboards e modelos de dados para diversas áreas, incluindo vendas, RH e faturamento. Além disso, atua como administrador do Workspace do Google da sua empresa. No tempo livre, tem interesse em música eletrônica e está organizando a festa Synapse. 🔗 Portfolio"
  },
  {
    "objectID": "about.html#davi-kemper",
    "href": "about.html#davi-kemper",
    "title": "Sobre Nós",
    "section": "Daniel K Junior",
    "text": "Daniel K Junior\n\nCargo: Formado na Escola de Sargento das Armas no ano de 2021, decidiu fazer a transição de carreira para a área de Dados já no ínicio da faculdade, concluindo a transição no final do ano de 2024, hoje atua como Analista de BI na EZ Chart.\n🔗 Portfolio"
  },
  {
    "objectID": "about.html#davi-kemper-1",
    "href": "about.html#davi-kemper-1",
    "title": "Sobre Nós",
    "section": "Davi Kemper",
    "text": "Davi Kemper\n\nCargo: Estudante de Ciência de Dados na FAE, atuou como Analista de BI do grupo Metronorte. 🔗 Portfolio"
  },
  {
    "objectID": "about.html#joão-niquele",
    "href": "about.html#joão-niquele",
    "title": "Sobre Nós",
    "section": "João Niquele",
    "text": "João Niquele\n\nCargo: Estudante de Ciência de Dados na FAE 🔗 Portfolio"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projeto Finanças",
    "section": "",
    "text": "Usaremos seguintes ações da bolsa :\n\nBRFS3: A BRF é uma empresa transnacional brasileira do ramo alimentício, fruto da fusão entre Sadia e Perdigão, duas das principais empresas de alimentos do Brasil.\nJBSS3: JBS é uma empresa brasileira do setor de alimentos fundada em 1953 em Goiás. A companhia opera no processamento de carnes bovina, suína, ovina, de frango, de peixe e plant-based, além de atuar no processamento de couros\nBEEF3: Minerva Foods é uma empresa brasileira de alimentos fundada em 1924 na cidade de Barretos. A companhia tem atuação na comercialização de carne in natura, couros, derivados, e na exportação de gado vivo, além de atuar no processamento de carnes.\nMRFG3: Marfrig Global Foods é uma empresa brasileira de alimentos. Fundada no ano 2000, é a segunda maior produtora de carne bovina do mundo e líder na produção de hambúrgueres.\nTSN: A Tyson Foods é uma empresa multinacional americana fundada por John W. Tyson em 1931 e sediada em Springdale, Arkansas, que opera na indústria alimentícia.\nHRL: A Hormel Foods Corporation é uma empresa alimentícia estadunidense com sede em Austin, Minnesota, conhecida pela fabricação do Spam. Em 24 de agosto de 2017, a empresa anunciou a compra da empresa brasileira Ceratti.\nGIS: General Mills é uma multinacional americana produtora de alimentos classificada na Fortune 500 e uma das 10 maiores empresas de alimentos do mundo. É sediada em Golden Valley, Minnesota, Minneapolis.\n\nUtilizamos a API Yahoo! Finance para conseguir os dados utilizados para as analises a seguir.\nAnalisando os dados em uma tabela:\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(cowplot) \nlibrary(lattice)\nlibrary(timetk)\nlibrary(quantmod)\nlibrary(DT) \n\n\nTICKERS &lt;- c(\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\n\nportfolioPrices &lt;- NULL\nfor ( Ticker in TICKERS )\n  portfolioPrices &lt;- cbind(\n    portfolioPrices, \n    getSymbols(\n      Ticker,\n      src = \"yahoo\",\n      from = \"2019-01-01\",\n      auto.assign = FALSE\n    )[,4]\n  )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"BRFS3\",\n  \"JBSS3\",\n  \"BEEF3\",\n  \"MRFG3\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\nCode\n# Visualizar com DT\ndatatable(tail(portfolioPrices), options = list(pageLength = 10, scrollX = TRUE)) \n\n\n\n\n\n\nE então a gente faz uma analise temporal dos dados, tendo o eixo X sendo a variável tempo, e o eixo Y sendo o preço:\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along(GIS)\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n  ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )"
  },
  {
    "objectID": "Projeto Finanças/index.html",
    "href": "Projeto Finanças/index.html",
    "title": "Projeto Finanças",
    "section": "",
    "text": "Usaremos seguintes ações da bolsa :\n\nBRFS3: A BRF é uma empresa transnacional brasileira do ramo alimentício, fruto da fusão entre Sadia e Perdigão, duas das principais empresas de alimentos do Brasil.\nJBSS3: JBS é uma empresa brasileira do setor de alimentos fundada em 1953 em Goiás. A companhia opera no processamento de carnes bovina, suína, ovina, de frango, de peixe e plant-based, além de atuar no processamento de couros\nBEEF3: Minerva Foods é uma empresa brasileira de alimentos fundada em 1924 na cidade de Barretos. A companhia tem atuação na comercialização de carne in natura, couros, derivados, e na exportação de gado vivo, além de atuar no processamento de carnes.\nMRFG3: Marfrig Global Foods é uma empresa brasileira de alimentos. Fundada no ano 2000, é a segunda maior produtora de carne bovina do mundo e líder na produção de hambúrgueres.\nTSN: A Tyson Foods é uma empresa multinacional americana fundada por John W. Tyson em 1931 e sediada em Springdale, Arkansas, que opera na indústria alimentícia.\nHRL: A Hormel Foods Corporation é uma empresa alimentícia estadunidense com sede em Austin, Minnesota, conhecida pela fabricação do Spam. Em 24 de agosto de 2017, a empresa anunciou a compra da empresa brasileira Ceratti.\nGIS: General Mills é uma multinacional americana produtora de alimentos classificada na Fortune 500 e uma das 10 maiores empresas de alimentos do mundo. É sediada em Golden Valley, Minnesota, Minneapolis.\n\nUtilizamos a API Yahoo! Finance para conseguir os dados utilizados para as analises a seguir.\nAnalisando os dados em uma tabela:\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(cowplot) \nlibrary(lattice)\nlibrary(timetk)\nlibrary(quantmod)\nlibrary(DT) \n\n\nTICKERS &lt;- c(\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\n\nportfolioPrices &lt;- NULL\nfor ( Ticker in TICKERS )\n  portfolioPrices &lt;- cbind(\n    portfolioPrices, \n    getSymbols(\n      Ticker,\n      src = \"yahoo\",\n      from = \"2019-01-01\",\n      auto.assign = FALSE\n    )[,4]\n  )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"BRFS3\",\n  \"JBSS3\",\n  \"BEEF3\",\n  \"MRFG3\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\nCode\n# Visualizar com DT\ndatatable(tail(portfolioPrices), options = list(pageLength = 10, scrollX = TRUE)) \n\n\n\n\n\n\nE então a gente faz uma analise temporal dos dados, tendo o eixo X sendo a variável tempo, e o eixo Y sendo o preço:\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along(GIS)\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n  ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )"
  },
  {
    "objectID": "page2.html",
    "href": "page2.html",
    "title": "Ciência de Dados para Negócios: Big Data for Finance Project",
    "section": "",
    "text": "Resumo\n\n\n\n\nteste de futuro para as ações\n\n\n\n\nIntro\nescrever\n\nR\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(purrr)\nlibrary(tidyquant)\nlibrary(tsibble)\nlibrary(prophet)\nlibrary(feasts)\nlibrary(fable)\nlibrary(fabletools)\nlibrary(lubridate)\nlibrary(tictoc)\n\n\nCarregamos os dados:\n\n\nCode\ntickers &lt;- c(\n         \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\nEntão baixo os dados via Yahoo!Finance:\n\n\nCode\nportfolioPrices &lt;- NULL\n  for ( Ticker in tickers )\n    portfolioPrices &lt;- cbind(\n      portfolioPrices, \n      quantmod::getSymbols.yahoo(\n        Ticker,\n        from = \"2019-01-01\",\n        auto.assign = FALSE\n      )[,4]\n    )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n# Visualizar com DT\n#DT::datatable(tail(portfolioPrices), options = list(pageLength = 10, scrollX = TRUE)) \n\n\nVisualizando os dados dos nossos últimos retornos dos preços, temos:\n\n\nCode\nlog_returns &lt;- log(portfolioPrices) - log(lag(portfolioPrices))\nlog_returns &lt;- na.omit(log_returns)\nlog_returns &lt;- log_returns |&gt; \n  timetk::tk_tbl(preserve_index = TRUE, rename_index = \"date\")\n\ntail(log_returns)\n\n\n\n  \n\n\n\n\n\nCode\nln_returns &lt;- log_returns\n\nln_returns |&gt; as.data.frame() |&gt;\n  dplyr::mutate(\n    time = seq_along( TSN )\n  ) |&gt; select(-date) |&gt;\n  tidyr::pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  dplyr::group_by(Variables) |&gt;\n  timetk::plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  ggplot2::theme(\n    strip.background = ggplot2::element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\n\nModelagem com fpp3 e validação cruzada temporal\nPrecisaremos fazer um forecasting de curto prazo com nossos dados históricos de retornos pra formularmos nossas recomendações posteriores de compra, venda e espera:\n\nVamos começar com uma série por vez \\(\\Rightarrow\\) TSN\n\n\n\nCode\n# Primeiro converto pra tsibble\n\nlnretTSN &lt;- log_returns |&gt; \n  select(date, TSN) |&gt; \n  as_tsibble(index = date)\n\nglimpse(lnretTSN)\n\n\nRows: 1,519\nColumns: 2\n$ date &lt;date&gt; 2019-01-03, 2019-01-04, 2019-01-07, 2019-01-08, 2019-01-09, 2019…\n$ TSN  &lt;dbl&gt; 0.0211432803, 0.0122208208, 0.0160060857, 0.0264099860, -0.017528…\n\n\n\n\nCode\ntreino &lt;- lnretTSN |&gt;\n  filter_index(~\"2025-01-01\")\n\n\nWar models\n\n\nCode\ntic()\n\nModelos &lt;- treino |&gt;\n  model(\n    AjusteExp = ETS(TSN ~ error(\"A\") + trend(\"N\") + season(\"N\")), # Ajuste Exponencial com auto\n    \n    AjExp_aditivo = ETS(TSN ~ error(\"A\") + trend(\"A\") + season(\"A\")), # Ajuste Exponencial Aditivo\n    \n    AjExp_multiplicativo = ETS(TSN ~ error(\"M\") + trend(\"A\") + season(\"M\")), # Ajuste Exponencial Multiplicativo\n    \n    Croston = CROSTON(TSN), # Modelo Croston\n    \n    HoltWinters = ETS(TSN ~ error(\"M\") + trend(\"Ad\") + season(\"M\")), # Holt Winters\n    \n    Holt = ETS(TSN ~ error(\"A\") + trend(\"A\") + season(\"N\")), # Holt\n    \n    HoltAmort = ETS(TSN ~ error(\"A\") + trend(\"Ad\", phi = 0.9) + season(\"N\")), # Holt Amortecida\n    \n    Regr_Comp = TSLM(TSN ~ trend() + season()), # Regressao com tendencia e sazonalidade auto\n    \n    Regr_Harmonica = TSLM(TSN ~ trend() + fourier(K = 2)), # Regressao harmonica\n    \n    Regr_Quebras = TSLM(TSN ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n    \n    Snaive = SNAIVE(TSN), # SNAIVE\n    \n    Naive = NAIVE(TSN), #NAIVE\n    \n    Media_Movel = ARIMA(TSN ~ pdq(0,0,1)), # Media Movel Simples\n    \n    autoARIMA = ARIMA(TSN, stepwise = FALSE, approx = FALSE), # Auto ARIMA\n    \n    autoARIMA_saz = ARIMA(TSN, stepwise = FALSE, approx = FALSE, seasonal = TRUE), # AutoARIMA Sazonal\n    \n    #    Regr_erros_ARIMA = auto.arima(TSN, xreg = fourier(K = 3), seasonal = FALSE), # Regressao com erros ARIMA\n    \n    ARIMA_saz_012011 = ARIMA(TSN ~ pdq(0,1,2) + PDQ(0,1,1)), # ARIMA Sazonal ordem 012011\n    \n    ARIMA_saz_210011 = ARIMA(TSN ~ pdq(2,1,0) + PDQ(0,1,1)), # ARIMA Sazonal ordem 210011\n    \n    ARIMA_saz_0301012 = ARIMA(TSN ~ 0 + pdq(3,0,1) + PDQ(0,1,2)), # ARIMA sazonal\n    \n    ARIMA_quad = ARIMA(TSN ~ I(trend()^2)), # ARIMA com tendencia temporal quadratica\n    \n    ARIMA_determ = ARIMA(TSN ~ 1 + trend() + pdq(d = 0)), # ARIMA com tendencia deterministica\n    \n    ARIMA_estocastico = ARIMA(TSN ~ pdq(d = 1)), # ARIMA com tendência estocastica\n    \n    Regr_Harm_dinamica = ARIMA(TSN ~ fourier(K=2) + PDQ(0,0,0)), # Regressao Harmonica Dinamica\n    \n    Regr_Harm_Din_MultSaz = ARIMA(TSN ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = 7*30, K = 10) + fourier(period = 7*30, K = 5)), \n    \n    Regr_Harm_Din_Saz = ARIMA(TSN ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = \"month\", K = 10) +\n                                fourier(period = \"year\", K = 2) ), # Rgr Harm Mult Saz Complexa\n    \n#    Auto_Prophet = prophet(TSN), # Auto prophet\n    \n#    Prophet_mult = prophet(TSN ~ season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_aditivo = prophet(TSN ~ season(period = \"month\", order = 2, type = \"additive\")),\n    \n#    Prophet_geom = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_memo = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 5) +\n#                             season(period = \"year\", order = 2, type = \"multiplicative\")),\n    \n    Modelo_VAR = VAR(TSN, ic = \"bic\"), # Vetor Autoregressivo \n    \n    Random_Walk = RW(TSN ~ drift()), # Random Walk com drift\n    \n    Rede_Neural_AR = NNETAR(TSN, bootstrap =  TRUE)#, # Rede Neural com auto AR e bootstraping nos erros\n    \n    #    x11 = X_13ARIMA_SEATS(TSN ~ x11()) # X11 ARIMA Seats\n    \n  ) |&gt;\n  \n  forecast(h = \"24 months\") # Horizonte de projecao para os proximos 30 dias apos corte no treino\n\ntoc()  \n\n\n0.93 sec elapsed\n\n\nSelecionamos o melhor modelo (1 fold de validação cruzada somente):\n\n\nCode\nModelos |&gt;\n  accuracy(lnretTSN) |&gt;\n  arrange(RMSE) # Seleção da acuracia pelo menor RMSE para o conjunto de modelos\n\n\n\n  \n\n\n\nGero um cenário com o modelo:\n\n\nCode\nfit &lt;- lnretTSN |&gt;\n  model(\n    Regr_Quebras = TSLM(TSN ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n  )\n\nsim &lt;- fit |&gt; generate(h = 30, times = 5, bootstrap = TRUE)\n\n\nPlotamos os forecasts com esse modelo pra três cenários distintos no futuro:\n\n\nCode\nlnretTSN |&gt;\n  filter_index(\"2025-01-01\"~.) |&gt;\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = TSN)) +\n  geom_line(aes(y = .sim, colour = as.factor(.rep)),\n    data = sim) +\n  labs(title=\"Valores projetados de retornos de preços de contratos futuros da TSN\", y=\"$US\" ) +\n  guides(colour = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Primeiro converto pra tsibble\n\nlnretGIS &lt;- log_returns |&gt; \n  select(date, GIS) |&gt; \n  as_tsibble(index = date)\n\nglimpse(lnretGIS)\n\n\nRows: 1,519\nColumns: 2\n$ date &lt;date&gt; 2019-01-03, 2019-01-04, 2019-01-07, 2019-01-08, 2019-01-09, 2019…\n$ GIS  &lt;dbl&gt; 0.0154921374, 0.0200387411, 0.0164387227, 0.0149567729, -0.016440…\n\n\n\n\nCode\ntreino &lt;- lnretGIS |&gt;\n  filter_index(~\"2025-01-01\")\n\n\n\n\nCode\ntic()\n\nModelos &lt;- treino |&gt;\n  model(\n    AjusteExp = ETS(GIS ~ error(\"A\") + trend(\"N\") + season(\"N\")), # Ajuste Exponencial com auto\n    \n    AjExp_aditivo = ETS(GIS ~ error(\"A\") + trend(\"A\") + season(\"A\")), # Ajuste Exponencial Aditivo\n    \n    AjExp_multiplicativo = ETS(GIS ~ error(\"M\") + trend(\"A\") + season(\"M\")), # Ajuste Exponencial Multiplicativo\n    \n    Croston = CROSTON(GIS), # Modelo Croston\n    \n    HoltWinters = ETS(GIS ~ error(\"M\") + trend(\"Ad\") + season(\"M\")), # Holt Winters\n    \n    Holt = ETS(GIS ~ error(\"A\") + trend(\"A\") + season(\"N\")), # Holt\n    \n    HoltAmort = ETS(GIS ~ error(\"A\") + trend(\"Ad\", phi = 0.9) + season(\"N\")), # Holt Amortecida\n    \n    Regr_Comp = TSLM(GIS ~ trend() + season()), # Regressao com tendencia e sazonalidade auto\n    \n    Regr_Harmonica = TSLM(GIS ~ trend() + fourier(K = 2)), # Regressao harmonica\n    \n    Regr_Quebras = TSLM(GIS ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n    \n    Snaive = SNAIVE(GIS), # SNAIVE\n    \n    Naive = NAIVE(GIS), #NAIVE\n    \n    Media_Movel = ARIMA(GIS ~ pdq(0,0,1)), # Media Movel Simples\n    \n    autoARIMA = ARIMA(GIS, stepwise = FALSE, approx = FALSE), # Auto ARIMA\n    \n    autoARIMA_saz = ARIMA(GIS, stepwise = FALSE, approx = FALSE, seasonal = TRUE), # AutoARIMA Sazonal\n    \n    #    Regr_erros_ARIMA = auto.arima(TSN, xreg = fourier(K = 3), seasonal = FALSE), # Regressao com erros ARIMA\n    \n    ARIMA_saz_012011 = ARIMA(GIS ~ pdq(0,1,2) + PDQ(0,1,1)), # ARIMA Sazonal ordem 012011\n    \n    ARIMA_saz_210011 = ARIMA(GIS ~ pdq(2,1,0) + PDQ(0,1,1)), # ARIMA Sazonal ordem 210011\n    \n    ARIMA_saz_0301012 = ARIMA(GIS ~ 0 + pdq(3,0,1) + PDQ(0,1,2)), # ARIMA sazonal\n    \n    ARIMA_quad = ARIMA(GIS ~ I(trend()^2)), # ARIMA com tendencia temporal quadratica\n    \n    ARIMA_determ = ARIMA(GIS ~ 1 + trend() + pdq(d = 0)), # ARIMA com tendencia deterministica\n    \n    ARIMA_estocastico = ARIMA(GIS ~ pdq(d = 1)), # ARIMA com tendência estocastica\n    \n    Regr_Harm_dinamica = ARIMA(GIS ~ fourier(K=2) + PDQ(0,0,0)), # Regressao Harmonica Dinamica\n    \n    Regr_Harm_Din_MultSaz = ARIMA(GIS ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = 7*30, K = 10) + fourier(period = 7*30, K = 5)), \n    \n    Regr_Harm_Din_Saz = ARIMA(GIS ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = \"month\", K = 10) +\n                                fourier(period = \"year\", K = 2) ), # Rgr Harm Mult Saz Complexa\n    \n#    Auto_Prophet = prophet(TSN), # Auto prophet\n    \n#    Prophet_mult = prophet(TSN ~ season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_aditivo = prophet(TSN ~ season(period = \"month\", order = 2, type = \"additive\")),\n    \n#    Prophet_geom = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_memo = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 5) +\n#                             season(period = \"year\", order = 2, type = \"multiplicative\")),\n    \n    Modelo_VAR = VAR(GIS, ic = \"bic\"), # Vetor Autoregressivo \n    \n    Random_Walk = RW(GIS ~ drift()), # Random Walk com drift\n    \n    Rede_Neural_AR = NNETAR(GIS, bootstrap =  TRUE)#, # Rede Neural com auto AR e bootstraping nos erros\n    \n    #    x11 = X_13ARIMA_SEATS(TSN ~ x11()) # X11 ARIMA Seats\n    \n  ) |&gt;\n  \n  forecast(h = \"24 months\") # Horizonte de projecao para os proximos 30 dias apos corte no treino\n\ntoc()  \n\n\n0.83 sec elapsed\n\n\n\n\nCode\nfit &lt;- lnretGIS |&gt;\n  model(\n    Regr_Quebras = TSLM(GIS ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n  )\n\nsim &lt;- fit |&gt; generate(h = 30, times = 5, bootstrap = TRUE)\n\n\n\n\nCode\nlnretTSN |&gt;\n  filter_index(\"2025-01-01\"~.) |&gt;\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = TSN)) +\n  geom_line(aes(y = .sim, colour = as.factor(.rep)),\n    data = sim) +\n  labs(title=\"Valores projetados de retornos de preços de contratos futuros da SALESFORCE\", y=\"$US\" ) +\n  guides(colour = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\nReferences\n\nMarkowitz, H. (1952). Portfolio Selection. The Journal of Finance, 7(1), 77–91.\nLink\nSharpe, W. F. (1966). Mutual Fund Performance. The Journal of Business, 39(1), 119–138.\nLink\nElton, E. J., Gruber, M. J., Brown, S. J., & Goetzmann, W. N. (2007). Modern Portfolio Theory and Investment Analysis (9th ed.). Wiley.\nHilpisch, Y. (2018). Python for Finance: Mastering Data-Driven Finance. O’Reilly Media."
  }
]
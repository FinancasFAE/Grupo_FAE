[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projeto Finan√ßas",
    "section": "",
    "text": "Usaremos seguintes a√ß√µes da bolsa :\n\nBRFS3: A BRF √© uma empresa transnacional brasileira do ramo aliment√≠cio, fruto da fus√£o entre Sadia e Perdig√£o, duas das principais empresas de alimentos do Brasil.\nJBSS3: JBS √© uma empresa brasileira do setor de alimentos fundada em 1953 em Goi√°s. A companhia opera no processamento de carnes bovina, su√≠na, ovina, de frango, de peixe e plant-based, al√©m de atuar no processamento de couros\nBEEF3: Minerva Foods √© uma empresa brasileira de alimentos fundada em 1924 na cidade de Barretos. A companhia tem atua√ß√£o na comercializa√ß√£o de carne in natura, couros, derivados, e na exporta√ß√£o de gado vivo, al√©m de atuar no processamento de carnes.\nMRFG3: Marfrig Global Foods √© uma empresa brasileira de alimentos. Fundada no ano 2000, √© a segunda maior produtora de carne bovina do mundo e l√≠der na produ√ß√£o de hamb√∫rgueres.\nTSN: A Tyson Foods √© uma empresa multinacional americana fundada por John W. Tyson em 1931 e sediada em Springdale, Arkansas, que opera na ind√∫stria aliment√≠cia.\nHRL: A Hormel Foods Corporation √© uma empresa aliment√≠cia estadunidense com sede em Austin, Minnesota, conhecida pela fabrica√ß√£o do Spam. Em 24 de agosto de 2017, a empresa anunciou a compra da empresa brasileira Ceratti.\nGIS: General Mills √© uma multinacional americana produtora de alimentos classificada na Fortune 500 e uma das 10 maiores empresas de alimentos do mundo. √â sediada em Golden Valley, Minnesota, Minneapolis.\n\nUtilizamos a API Yahoo! Finance para conseguir os dados utilizados para as analises a seguir.\nAnalisando os dados em uma tabela:\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(cowplot) \nlibrary(lattice)\nlibrary(timetk)\nlibrary(quantmod)\nlibrary(DT) \n\n\nTICKERS &lt;- c(\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\n\nportfolioPrices &lt;- NULL\nfor ( Ticker in TICKERS )\n  portfolioPrices &lt;- cbind(\n    portfolioPrices, \n    getSymbols(\n      Ticker,\n      src = \"yahoo\",\n      from = \"2019-01-01\",\n      auto.assign = FALSE\n    )[,4]\n  )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"BRFS3\",\n  \"JBSS3\",\n  \"BEEF3\",\n  \"MRFG3\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\nCode\n# Visualizar com DT\ndatatable(tail(portfolioPrices), options = list(pageLength = 10, scrollX = TRUE)) \n\n\n\n\n\n\nE ent√£o a gente faz uma analise temporal dos dados, tendo o eixo X sendo a vari√°vel tempo, e o eixo Y sendo o pre√ßo:\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along(GIS)\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n  ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre N√≥s",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJo√£o Niquele"
  },
  {
    "objectID": "about.html#sum√°rio",
    "href": "about.html#sum√°rio",
    "title": "Sobre N√≥s",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJo√£o Niquele"
  },
  {
    "objectID": "about.html#arthur-lauffer",
    "href": "about.html#arthur-lauffer",
    "title": "Sobre N√≥s",
    "section": "Arthur Lauffer",
    "text": "Arthur Lauffer\n\nCargo: √â analista de BI e estudante de Ci√™ncia de Dados para Neg√≥cios na FAE Business School. Ele administra sua pr√≥pria empresa de BI, prestando servi√ßos para outras empresas, e tamb√©m gerencia uma empresa de SaaS focada em projetos de longo prazo. Com grande experi√™ncia em Power BI, ele desenvolve dashboards e modelos de dados para diversas √°reas, incluindo vendas, RH e faturamento. Al√©m disso, atua como administrador do Workspace do Google da sua empresa. No tempo livre, tem interesse em m√∫sica eletr√¥nica e est√° organizando a¬†festa¬†Synapse. üîó Portfolio"
  },
  {
    "objectID": "about.html#davi-kemper",
    "href": "about.html#davi-kemper",
    "title": "Sobre N√≥s",
    "section": "Daniel K Junior",
    "text": "Daniel K Junior\n\nCargo: Formado na Escola de Sargento das Armas no ano de 2021, decidiu fazer a transi√ß√£o de carreira para a √°rea de Dados j√° no √≠nicio da faculdade, concluindo a transi√ß√£o no final do ano de 2024, hoje atua como Analista de BI na EZ Chart.\nüîó Portfolio"
  },
  {
    "objectID": "about.html#davi-kemper-1",
    "href": "about.html#davi-kemper-1",
    "title": "Sobre N√≥s",
    "section": "Davi Kemper",
    "text": "Davi Kemper\n\nCargo: Estudante de Ci√™ncia de Dados na FAE, atuou como Analista de BI do grupo Metronorte. üîó Portfolio"
  },
  {
    "objectID": "about.html#jo√£o-niquele",
    "href": "about.html#jo√£o-niquele",
    "title": "Sobre N√≥s",
    "section": "Jo√£o Niquele",
    "text": "Jo√£o Niquele\n\nCargo: Estudante de Ci√™ncia de Dados na FAE üîó Portfolio"
  },
  {
    "objectID": "FeatEngFinancial.html#introdu√ß√£o-feature-engineering-em-dados-de-s√©ries-financeiras",
    "href": "FeatEngFinancial.html#introdu√ß√£o-feature-engineering-em-dados-de-s√©ries-financeiras",
    "title": "Feature Engineering com s√©ries de pre√ßos de ativos financeiros",
    "section": "üìå Introdu√ß√£o: Feature Engineering em dados de s√©ries financeiras",
    "text": "üìå Introdu√ß√£o: Feature Engineering em dados de s√©ries financeiras\n\n\nMedindo a Volatilidade: Com e Sem GARCH\nA abordagem tradicional calcula a volatilidade como o desvio padr√£o dos retornos hist√≥ricos em uma janela m√≥vel de tamanho \\(N\\): (dias de negocia√ß√£o)\n\\[\n\\sigma_{\\text{hist}} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(r_i - \\bar{r})^2}\n\\]\nVantagens:\n\nSimplicidade e facilidade de implementa√ß√£o.\n\nDesvantagens:\n\nAssume volatilidade constante durante a janela. (ou seja precisa de um range dias e n√£o √© capaz de medir o desvio ou o risco/volatilidade de um dia pro outro.)\nN√£o capta a persist√™ncia dos choques (efeito de clustering).\nN√£o reage dinamicamente a choques recentes.\n\n\n\nVolatilidade com o GARCH\nO modelo GARCH(1,1) estima a vari√¢ncia condicional de forma din√¢mica:\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2.\n\\]\nOnde: - \\(\\epsilon_{t-1}^2\\) reflete o impacto dos choques recentes. - \\(\\sigma_{t-1}^2\\) reflete a persist√™ncia da volatilidade do per√≠odo anterior. - \\(\\omega &gt; 0\\), \\(\\alpha \\geq 0\\) e \\(\\beta \\geq 0\\) s√£o par√¢metros estimados.\nA soma \\(\\alpha + \\beta\\) mede a persist√™ncia total da volatilidade:\n\nPr√≥ximo de 1: Choques t√™m efeitos duradouros; a volatilidade permanece alta por v√°rios per√≠odos.\nMenor que 1: Os choques se dissipam mais rapidamente; a volatilidade retorna ao seu n√≠vel m√©dio mais r√°pido.\n\nVantagens do GARCH:\n\nModela a volatilidade de forma din√¢mica.\nCaptura o efeito de ‚Äúclustering‚Äù dos choques.\nPermite previs√µes mais precisas da volatilidade futura.\n\nDesvantagens do GARCH:\n\nRequer estima√ß√£o de par√¢metros e pressup√µe uma estrutura espec√≠fica para a volatilidade.\nPode ser sens√≠vel √† escolha da distribui√ß√£o dos res√≠duos (por exemplo, normal vs.¬†\\(t\\) de Student).\n\nVolatilidade com desvio-padr√£oVolatilidade com GARCH(1,1)Plotando retorno x risco\n\n\nA volatilidade hist√≥rica pode ser medida como o desvio-padr√£o dos log-retornos calculado em uma janela m√≥vel de \\(N\\) dias de negocia√ß√£o.\nNeste exemplo, adotamos uma janela m√≥vel de 5 dias (aproximadamente uma semana de negocia√ß√£o. Para um m√™s de negocia√ß√£o, utilizar 22 e um ano 252) para capturar a volatilidade di√°ria dos ativos.\nVantagens:\n\nSimplicidade e facilidade de implementa√ß√£o.\n\nDesvantagens:\n\nAssume volatilidade constante durante a janela.\nN√£o capta a persist√™ncia dos choques.\nN√£o reage dinamicamente a choques recentes.\n\n\n\nCode\n# Calcular a volatilidade hist√≥rica com uma janela m√≥vel de 5 dias\n\n# Supondo que 'log_returns' j√° foi calculado e possui a coluna 'date' e os log-retornos dos ativos\nwindow = 5\n\n# Cria um DataFrame para armazenar a volatilidade hist√≥rica\nvol_hist = pd.DataFrame({'date': log_returns[\"date\"]})\n\n# Calcula o desvio-padr√£o m√≥vel (volatilidade) para cada ativo\nfor col in log_returns.columns[1:]:\n    vol_hist[col] = log_returns[col].rolling(window=window).std()\n\n# Exibe as ultimas linhas do DataFrame de volatilidade hist√≥rica\n#print(vol_hist.head()) # 5 primeiros ser√£o NaN\nprint(vol_hist.tail())\n\n\n            date  BEEF3.SA  BRFS3.SA       GIS       HRL  JBSS3.SA  MRFG3.SA  \\\n1238  2025-03-14  0.031093  0.024722  0.024976  0.014724  0.015667  0.024863   \n1239  2025-03-17  0.031816  0.026581  0.028208  0.015502  0.016132  0.028757   \n1240  2025-03-18  0.022213  0.040627  0.021501  0.014557  0.073009  0.036127   \n1241  2025-03-19  0.021905  0.039398  0.015938  0.007907  0.073728  0.035161   \n1242  2025-03-20  0.033103  0.034645  0.016877  0.007681  0.071088  0.039908   \n\n           TSN  \n1238  0.011254  \n1239  0.011458  \n1240  0.011033  \n1241  0.006089  \n1242  0.006956  \n\n\nPlotando temos:\n\n\nCode\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\n# Certificar que \"date\" √© datetime (j√° feito)\nvol_hist['date'] = pd.to_datetime(vol_hist['date'])\n\n# Transformar para formato longo\nvol_hist_long = vol_hist.melt(id_vars='date', var_name='Ativo', value_name='Volatilidade_Hist')\n\nfig = px.line(vol_hist_long, x='date', y='Volatilidade_Hist', color='Ativo',\n              title='Volatilidade Hist√≥rica (Desvio-Padr√£o) com janela de 5 dias')\nfig.show()\n\n\n\n\nO desvio-padr√£o assume que precisaremos de um range maior do que 2 pontos no tempo, o que limita nossa an√°lise pois a incompatibiliza, uma vez que precisaremos comparar os riscos di√°rios x retornos di√°rios (como feito anteriormente). Ou seja, n√£o podemos comparar retornos dia-a-dia x volatilidades (risco) de 5 em 5 dias p.¬†ex.\nOs modelos heteroced√°sticos (da fam√≠lia ARCH) estimam a vari√¢ncia condicional dos nossos dados, ou seja, em linguagem de finan√ßas, eles s√£o capazes de capturar as volatilidades ou risco dos retornos dos pre√ßos de ativos financeiros ponto a ponto no tempo, ou seja, dia a dia.\nO modelo GARCH(1,1) com distribui√ß√£o \\(t\\) assim√©trica n√£o est√° dispon√≠vel diretamente na maioria das bibliotecas Python. No entanto, podemos utilizar um GARCH(1,1) com uma distribui√ß√£o \\(t\\) padr√£o para estimar a vari√¢ncia condicional. O modelo √© representado por:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nOnde:\n\n\\(r_t\\) √© o log-retorno no tempo \\(t\\).\n\\(\\mu\\) √© a m√©dia dos retornos.\n\\(\\epsilon_t\\) √© o termo de erro, condicionado √†s informa√ß√µes passadas.\n\\(\\sigma_t^2\\) √© a vari√¢ncia condicional no tempo \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) s√£o os par√¢metros a serem estimados, com \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) segue uma distribui√ß√£o \\(t\\) de Student com \\(ŒΩ\\) graus de liberdade para capturar as caudas pesadas observadas em retornos financeiros.\n\nA soma \\(\\alpha + \\beta\\) √© frequentemente utilizada para medir a persist√™ncia da volatilidade: quanto mais pr√≥ximos de 1, maior a persist√™ncia dos choques na volatilidade.\nVamos estimar a vari√¢ncia condicional (\\(\\sigma^2_{t}\\) ) para cada ativo:\n\n\nCode\n# Estimar o modelo GARCH(1,1) e salvar vari√¢ncia condicional\nvar_condicional = pd.DataFrame({\"date\": log_returns[\"date\"]})\n\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n    var_condicional[col] = res.conditional_volatility ** 2\n\nvar_condicional.head()\n\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007185. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009805. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 8. The message is:\nPositive directional derivative for linesearch\nSee scipy.optimize.fmin_slsqp for code meaning.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001789. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002188. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000497. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000827. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000305. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\n\n\n\n\n\n\n\ndate\nBEEF3.SA\nBRFS3.SA\nGIS\nHRL\nJBSS3.SA\nMRFG3.SA\nTSN\n\n\n\n\n1\n2020-03-24\n0.001096\n0.012336\n0.001097\n0.001420\n0.005637\n0.001305\n0.001633\n\n\n2\n2020-03-25\n0.000934\n0.029594\n0.001012\n0.006134\n0.011258\n0.001398\n0.002033\n\n\n3\n2020-03-26\n0.000837\n0.046902\n0.000876\n0.002925\n0.005405\n0.001291\n0.002510\n\n\n4\n2020-03-27\n0.001090\n0.065082\n0.002087\n0.007534\n0.004224\n0.001003\n0.002688\n\n\n5\n2020-03-30\n0.000888\n0.084714\n0.001521\n0.000928\n0.005940\n0.000853\n0.002965\n\n\n\n\n\n\n\nVamos avaliar os par√¢metros estimados do modelo:\n\n\nCode\n# Inferir sobre os par√¢metros do modelo GARCH(1,1) para cada ativo do portf√≥lio\n\nparams_list = []\n\n# Iterar sobre cada ativo (exceto a coluna 'date')\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n\n    par = res.params\n    alpha_val = par.get(\"alpha[1]\", None)\n    beta_val  = par.get(\"beta[1]\", None)\n    alpha_beta_sum = (alpha_val if alpha_val is not None else 0) + (beta_val if beta_val is not None else 0)\n\n    # Interpreta√ß√£o curta\n    if alpha_beta_sum &gt;= 0.9:\n        interp = f\"Alta persist√™ncia (Œ±+Œ≤ = {alpha_beta_sum:.4f}).\"\n    else:\n        interp = f\"Baixa/moderada persist√™ncia (Œ±+Œ≤ = {alpha_beta_sum:.4f}).\"\n\n    params_list.append({\n         \"Ativo\": col,\n         \"mu\": par.get(\"mu\", None),\n         \"omega\": par.get(\"omega\", None),\n         \"alpha\": alpha_val,\n         \"beta\": beta_val,\n         \"alpha+beta\": alpha_beta_sum,\n         \"nu\": par.get(\"nu\", None),\n         \"Interpretacao\": interp\n    })\n\ngarch_params = pd.DataFrame(params_list)\n\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007185. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009805. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 8. The message is:\nPositive directional derivative for linesearch\nSee scipy.optimize.fmin_slsqp for code meaning.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001789. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002188. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000497. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000827. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000305. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\nA soma \\(\\alpha + \\beta\\) √© um indicador crucial na modelagem GARCH para avaliar a persist√™ncia da volatilidade. Em termos pr√°ticos, os par√¢metros \\(\\alpha\\) e \\(\\beta\\) t√™m fun√ß√µes distintas:\n\n\\(\\alpha\\): Representa o impacto dos choques recentes (a inova√ß√£o ou termo de erro \\(\\epsilon_{t-1}^2\\) na volatilidade atual. Um valor mais alto de \\(\\alpha\\) indica que choques recentes t√™m um efeito maior em aumentar a volatilidade.\n\\(\\beta\\): Captura a persist√™ncia da volatilidade ao longo do tempo, ou seja, o efeito da volatilidade passada (\\(\\sigma_{t-1}^2)\\) sobre a volatilidade presente. Valores maiores de \\(\\beta\\) sugerem que a volatilidade tende a se manter elevada por um per√≠odo mais longo.\n\nQuando somamos esses dois par√¢metros, ou seja, quando calculamos \\(\\alpha + \\beta\\), obtemos uma medida da persist√™ncia total da volatilidade:\n\nSe \\(\\alpha + \\beta\\) estiver pr√≥ximo de 1, isso indica que os choques que afetam a volatilidade t√™m efeitos de longa dura√ß√£o. Em outras palavras, um choque na volatilidade tem um impacto que se dissipa muito lentamente, mantendo a volatilidade elevada por v√°rios per√≠odos.\nSe \\(\\alpha + \\beta\\) for significativamente menor que 1, os efeitos dos choques s√£o de curta dura√ß√£o e a volatilidade retorna rapidamente ao seu n√≠vel m√©dio ap√≥s um impacto.\n\nEm alguns casos, quando \\(\\alpha + \\beta = 1\\), o modelo √© denominado IGARCH (Integrated GARCH), o que implica que os choques t√™m efeitos persistentes permanentemente, ou seja, a volatilidade n√£o reverte para um valor m√©dio fixo.\nEsta caracter√≠stica √© particularmente importante na an√°lise de s√©ries financeiras, pois a persist√™ncia alta da volatilidade pode implicar maior risco de mercado e desafios na previs√£o dos retornos futuros. Assim, a soma \\(\\alpha + \\beta\\) serve como uma medida de ‚Äúmem√≥ria‚Äù dos choques, indicando se a volatilidade reage de forma passageira ou duradoura a eventos inesperados.\nGraficamente temos:\n\n\nCode\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BRFS3.SA']].copy()\nvol_zc = var_condicional[['date', 'BRFS3.SA']].copy()\n\n# Converter \"date\" para datetime, se necess√°rio\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Di√°rios - BRFS3.SA\", \"Volatilidade Condicional (GARCH) - BRFS3.SA\")\n)\n\n# Adicionar o gr√°fico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BRFS3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gr√°fico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BRFS3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BRFS3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()\n\n\nEm alguns casos, a vari√¢ncia condicional pode apresentar grandes oscila√ß√µes se houver outliers nos retornos ou problemas de converg√™ncia do modelo. Verifique:\n\nQualidade e limpeza dos dados\nResumo do ajuste (par√¢metros \\(\\alpha,\\beta\\) plaus√≠veis?)\nDistribui√ß√£o (\\(t\\) vs.¬†normal)\nModelos alternativos (EGARCH, GJR-GARCH, etc.)\n\n\n\nAqui iremos visualizar o comportamento do ativo ZC=F, futuros de milho:\n\n\nCode\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BRFS3.SA']].copy()\nvol_zc = var_condicional[['date', 'BRFS3.SA']].copy()\n\n# Converter \"date\" para datetime, se necess√°rio\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Di√°rios - BRFS3.SA\", \"Volatilidade Condicional (GARCH) - BRFS3.SA\")\n)\n\n# Adicionar o gr√°fico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BRFS3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gr√°fico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BRFS3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BRFS3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()\n\n\nNotem como o GARCH(1,1) captura bem os picos/quebras e na volatilidade dos retornos."
  }
]
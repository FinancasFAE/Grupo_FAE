[
  {
    "objectID": "terceiro_encontro.html",
    "href": "terceiro_encontro.html",
    "title": "Projeto Finan√ßas",
    "section": "",
    "text": "!pip install yahooquery\n\nCollecting yahooquery\n  Downloading yahooquery-2.3.7-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: beautifulsoup4&lt;5.0.0,&gt;=4.12.2 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (4.13.3)\nCollecting lxml&lt;5.0.0,&gt;=4.9.3 (from yahooquery)\n  Downloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: pandas&lt;3.0.0,&gt;=2.0.3 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (2.2.2)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.31.0 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (2.32.3)\nCollecting requests-futures&lt;2.0.0,&gt;=1.0.1 (from yahooquery)\n  Downloading requests_futures-1.0.2-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.65.0 in /usr/local/lib/python3.11/dist-packages (from yahooquery) (4.67.1)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.2-&gt;yahooquery) (2.6)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.2-&gt;yahooquery) (4.12.2)\nRequirement already satisfied: numpy&gt;=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2.0.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2025.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (2025.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3.0.0,&gt;=2.31.0-&gt;yahooquery) (2025.1.31)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&lt;3.0.0,&gt;=2.0.3-&gt;yahooquery) (1.17.0)\nDownloading yahooquery-2.3.7-py3-none-any.whl (52 kB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 52.7/52.7 kB 2.0 MB/s eta 0:00:00\nDownloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl (7.9 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7.9/7.9 MB 49.3 MB/s eta 0:00:00\nDownloading requests_futures-1.0.2-py2.py3-none-any.whl (7.7 kB)\nInstalling collected packages: lxml, requests-futures, yahooquery\n  Attempting uninstall: lxml\n    Found existing installation: lxml 5.3.1\n    Uninstalling lxml-5.3.1:\n      Successfully uninstalled lxml-5.3.1\nSuccessfully installed lxml-4.9.4 requests-futures-1.0.2 yahooquery-2.3.7\n\n\n\n!pip install arch\n\nCollecting arch\n  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy&gt;=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (2.0.2)\nRequirement already satisfied: scipy&gt;=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.14.1)\nRequirement already satisfied: pandas&gt;=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.2)\nRequirement already satisfied: statsmodels&gt;=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2025.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2025.1)\nRequirement already satisfied: patsy&gt;=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels&gt;=0.12-&gt;arch) (1.0.1)\nRequirement already satisfied: packaging&gt;=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels&gt;=0.12-&gt;arch) (24.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.4-&gt;arch) (1.17.0)\nDownloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 985.3/985.3 kB 8.9 MB/s eta 0:00:00\nInstalling collected packages: arch\nSuccessfully installed arch-7.2.0\n\n\n\n#import yfinance as yf\nfrom yahooquery import Ticker\nimport pandas as pd\nimport numpy as np\n#import matplotlib.pyplot as plt\n#import seaborn as sns\nfrom datetime import datetime\nfrom arch import arch_model # Lib do Python pra estimar as volatilidades (ARCH/GARCH)\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal\n\n\n# Tickers for portfolio\nTICKERS = [\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n]\n\n# Baixar os dados hist√≥ricos com yahooquery\ntickers = Ticker(TICKERS)\ndata = tickers.history(period=\"5y\")\n\n# Resetar o √≠ndice corretamente\ndata = data.reset_index()\n\n# O yahooquery retorna um MultiIndex, ent√£o √© preciso garantir que a coluna \"date\" exista corretamente\nif \"date\" not in data.columns:\n    raise ValueError(\"A coluna 'date' n√£o foi encontrada no dataset! Verifique a estrutura do DataFrame.\")\n\n# Selecionar apenas as colunas de interesse e reformatar\nportfolio_prices = data.pivot(index=\"date\", columns=\"symbol\", values=\"close\").reset_index()\n\n# Garantir que n√£o h√° valores ausentes\nportfolio_prices.dropna(inplace=True)\n\nportfolio_prices.head()\n\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n/usr/local/lib/python3.11/dist-packages/yahooquery/ticker.py:1333: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n\n\n  \n    \n\n\n\n\n\nsymbol\ndate\nBEEF3.SA\nBRFS3.SA\nGIS\nHRL\nJBSS3.SA\nMRFG3.SA\nTSN\n\n\n\n\n0\n2020-03-23\n8.20\n13.355227\n47.279999\n42.310001\n19.340000\n7.387873\n57.599998\n\n\n1\n2020-03-24\n8.40\n14.316651\n48.230000\n44.849998\n21.219999\n7.855208\n59.990002\n\n\n2\n2020-03-25\n8.19\n15.144808\n47.650002\n41.939999\n22.049999\n8.242997\n63.189999\n\n\n3\n2020-03-26\n7.70\n15.201922\n50.000000\n44.849998\n21.870001\n8.352373\n61.230000\n\n\n4\n2020-03-27\n7.80\n14.154827\n51.820000\n44.959999\n20.889999\n8.471693\n58.590000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\nimport plotly.express as px\n\n# Certifique-se de que a coluna \"date\" est√° em formato datetime\nportfolio_prices['date'] = pd.to_datetime(portfolio_prices['date'])\n\n# Transformar os pre√ßos para o formato longo (melt) para facilitar o plot\nprices_long = portfolio_prices.melt(id_vars='date', var_name='Ativo', value_name='Valor')\n\nfig = px.line(prices_long, x='date', y='Valor', color='Ativo',\n              title='S√©ries Temporais de Pre√ßos')\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n# Calcular os log-retornos\nlog_returns = portfolio_prices.copy()\nlog_returns.iloc[:, 1:] = np.log(portfolio_prices.iloc[:, 1:]).diff()\n\n# Remover a primeira linha que cont√©m NaN ap√≥s a diferencia√ß√£o\nlog_returns = log_returns.dropna()\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\n# Garantir que a coluna \"date\" esteja em formato datetime\nlog_returns['date'] = pd.to_datetime(log_returns['date'])\n\n# Transformar para formato longo\nlog_returns_long = log_returns.melt(id_vars='date', var_name='Ativo', value_name='Log_Retorno')\n\nfig = px.line(log_returns_long, x='date', y='Log_Retorno', color='Ativo',\n              title='S√©ries Temporais de Log-Retornos')\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Seleciona apenas as colunas dos ativos (excluindo a coluna \"date\")\nativos = log_returns.columns[1:]\n\n# Calcula a assimetria para cada ativo\nskewness = log_returns[ativos].skew()\n\n# Cria um DataFrame para visualizar os resultados\nskew_table = pd.DataFrame({\n    'Ativo': skewness.index,\n    'Skewness': skewness.values\n})\n\n# Adiciona a coluna que indica a dire√ß√£o da assimetria\nskew_table['Direcao'] = skew_table['Skewness'].apply(\n    lambda x: '√Ä direita' if x &gt; 0 else ('√Ä esquerda' if x &lt; 0 else 'Sim√©trica')\n)\n\n# Exibe a tabela atualizada\nskew_table\n\n\n  \n    \n\n\n\n\n\n\nAtivo\nSkewness\nDirecao\n\n\n\n\n0\nBEEF3.SA\n-0.322431\n√Ä esquerda\n\n\n1\nBRFS3.SA\n0.334181\n√Ä direita\n\n\n2\nGIS\n-0.307135\n√Ä esquerda\n\n\n3\nHRL\n-0.358598\n√Ä esquerda\n\n\n4\nJBSS3.SA\n0.526435\n√Ä direita\n\n\n5\nMRFG3.SA\n-0.079424\n√Ä esquerda\n\n\n6\nTSN\n-0.945498\n√Ä esquerda\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Retire o comando #| eval: false caso queira executar essa celula\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Transformar os log-retornos para formato longo (melt)\nlog_returns_long = log_returns.melt(id_vars=[\"date\"], var_name=\"Ativo\", value_name=\"Log_Retorno\")\n\n# Criar gr√°fico com Seaborn\nplt.figure(figsize=(12, 8))\ng = sns.FacetGrid(log_returns_long, col=\"Ativo\", col_wrap=2, sharex=False, sharey=False)\ng.map_dataframe(sns.histplot, x=\"Log_Retorno\", kde=True, bins=30, color=\"black\", alpha=0.5)\n\n# Ajustar t√≠tulo dos gr√°ficos\ng.set_titles(col_template=\"{col_name}\")\n\n# Melhorar layout\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# Calcular a volatilidade hist√≥rica com uma janela m√≥vel de 5 dias\n\n# Supondo que 'log_returns' j√° foi calculado e possui a coluna 'date' e os log-retornos dos ativos\nwindow = 5\n\n# Cria um DataFrame para armazenar a volatilidade hist√≥rica\nvol_hist = pd.DataFrame({'date': log_returns[\"date\"]})\n\n# Calcula o desvio-padr√£o m√≥vel (volatilidade) para cada ativo\nfor col in log_returns.columns[1:]:\n    vol_hist[col] = log_returns[col].rolling(window=window).std()\n\n# Exibe as ultimas linhas do DataFrame de volatilidade hist√≥rica\n#print(vol_hist.head()) # 5 primeiros ser√£o NaN\nprint(vol_hist.tail())\n\n           date  BEEF3.SA  BRFS3.SA       GIS       HRL  JBSS3.SA  MRFG3.SA  \\\n1238 2025-03-14  0.031093  0.024722  0.024976  0.014724  0.015667  0.024863   \n1239 2025-03-17  0.031816  0.026581  0.028208  0.015502  0.016132  0.028757   \n1240 2025-03-18  0.022213  0.040627  0.021501  0.014557  0.073009  0.036127   \n1241 2025-03-19  0.021905  0.039398  0.015938  0.007907  0.073728  0.035161   \n1242 2025-03-20  0.033103  0.034645  0.016877  0.007681  0.071088  0.039908   \n\n           TSN  \n1238  0.011254  \n1239  0.011458  \n1240  0.011033  \n1241  0.006089  \n1242  0.006956  \n\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\n# Certificar que \"date\" √© datetime (j√° feito)\nvol_hist['date'] = pd.to_datetime(vol_hist['date'])\n\n# Transformar para formato longo\nvol_hist_long = vol_hist.melt(id_vars='date', var_name='Ativo', value_name='Volatilidade_Hist')\n\nfig = px.line(vol_hist_long, x='date', y='Volatilidade_Hist', color='Ativo',\n              title='Volatilidade Hist√≥rica (Desvio-Padr√£o) com janela de 5 dias')\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Estimar o modelo GARCH(1,1) e salvar vari√¢ncia condicional\nvar_condicional = pd.DataFrame({\"date\": log_returns[\"date\"]})\n\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n    var_condicional[col] = res.conditional_volatility ** 2\n\nvar_condicional.head()\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007185. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009805. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001789. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:768: ConvergenceWarning:\n\nThe optimizer returned code 4. The message is:\nInequality constraints incompatible\nSee scipy.optimize.fmin_slsqp for code meaning.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002188. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000497. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000827. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000305. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\n\n\n  \n    \n\n\n\n\n\n\ndate\nBEEF3.SA\nBRFS3.SA\nGIS\nHRL\nJBSS3.SA\nMRFG3.SA\nTSN\n\n\n\n\n1\n2020-03-24\n0.001141\n0.008166\n0.000270\n0.001420\n0.005637\n0.001305\n0.002915\n\n\n2\n2020-03-25\n0.000994\n0.008166\n0.000148\n0.006134\n0.011258\n0.001398\n0.003002\n\n\n3\n2020-03-26\n0.000894\n0.008166\n0.000103\n0.002925\n0.005405\n0.001291\n0.003657\n\n\n4\n2020-03-27\n0.001153\n0.008166\n0.001017\n0.007534\n0.004224\n0.001003\n0.002935\n\n\n5\n2020-03-30\n0.000958\n0.008166\n0.000540\n0.000928\n0.005940\n0.000853\n0.003649\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n# Inferir sobre os par√¢metros do modelo GARCH(1,1) para cada ativo do portf√≥lio\n\nparams_list = []\n\n# Iterar sobre cada ativo (exceto a coluna 'date')\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n\n    par = res.params\n    alpha_val = par.get(\"alpha[1]\", None)\n    beta_val  = par.get(\"beta[1]\", None)\n    alpha_beta_sum = (alpha_val if alpha_val is not None else 0) + (beta_val if beta_val is not None else 0)\n\n    # Interpreta√ß√£o curta\n    if alpha_beta_sum &gt;= 0.9:\n        interp = f\"Alta persist√™ncia (Œ±+Œ≤ = {alpha_beta_sum:.4f}).\"\n    else:\n        interp = f\"Baixa/moderada persist√™ncia (Œ±+Œ≤ = {alpha_beta_sum:.4f}).\"\n\n    params_list.append({\n         \"Ativo\": col,\n         \"mu\": par.get(\"mu\", None),\n         \"omega\": par.get(\"omega\", None),\n         \"alpha\": alpha_val,\n         \"beta\": beta_val,\n         \"alpha+beta\": alpha_beta_sum,\n         \"nu\": par.get(\"nu\", None),\n         \"Interpretacao\": interp\n    })\n\ngarch_params = pd.DataFrame(params_list)\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007185. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009805. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001789. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:768: ConvergenceWarning:\n\nThe optimizer returned code 4. The message is:\nInequality constraints incompatible\nSee scipy.optimize.fmin_slsqp for code meaning.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002188. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000497. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000827. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n/usr/local/lib/python3.11/dist-packages/arch/univariate/base.py:309: DataScaleWarning:\n\ny is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.000305. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\n\n\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BEEF3.SA']].copy()\nvol_zc = var_condicional[['date', 'BEEF3.SA']].copy()\n\n# Converter \"date\" para datetime, se necess√°rio\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Di√°rios - BEEF3.SA\", \"Volatilidade Condicional (GARCH) - BEEF3.SA\")\n)\n\n# Adicionar o gr√°fico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BEEF3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gr√°fico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BEEF3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BEEF3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()"
  },
  {
    "objectID": "Projeto Finan√ßas/about.html",
    "href": "Projeto Finan√ßas/about.html",
    "title": "Sobre N√≥s",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJo√£o Niquele"
  },
  {
    "objectID": "Projeto Finan√ßas/about.html#sum√°rio",
    "href": "Projeto Finan√ßas/about.html#sum√°rio",
    "title": "Sobre N√≥s",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJo√£o Niquele"
  },
  {
    "objectID": "Projeto Finan√ßas/about.html#arthur-lauffer",
    "href": "Projeto Finan√ßas/about.html#arthur-lauffer",
    "title": "Sobre N√≥s",
    "section": "Arthur Lauffer",
    "text": "Arthur Lauffer\n\nCargo: √â analista de BI e estudante de Ci√™ncia de Dados para Neg√≥cios na FAE Business School. Ele administra sua pr√≥pria empresa de BI, prestando servi√ßos para outras empresas, e tamb√©m gerencia uma empresa de SaaS focada em projetos de longo prazo. Com grande experi√™ncia em Power BI, ele desenvolve dashboards e modelos de dados para diversas √°reas, incluindo vendas, RH e faturamento. Al√©m disso, atua como administrador do Workspace do Google da sua empresa. No tempo livre, tem interesse em m√∫sica eletr√¥nica e est√° organizando a¬†festa¬†Synapse. üîó Portfolio"
  },
  {
    "objectID": "Projeto Finan√ßas/about.html#davi-kemper",
    "href": "Projeto Finan√ßas/about.html#davi-kemper",
    "title": "Sobre N√≥s",
    "section": "Daniel K Junior",
    "text": "Daniel K Junior\n\nCargo: Formado na Escola de Sargento das Armas no ano de 2021, decidiu fazer a transi√ß√£o de carreira para a √°rea de Dados j√° no √≠nicio da faculdade, concluindo a transi√ß√£o no final do ano de 2024, hoje atua como Analista de BI na EZ Chart.\nüîó Portfolio"
  },
  {
    "objectID": "Projeto Finan√ßas/about.html#davi-kemper-1",
    "href": "Projeto Finan√ßas/about.html#davi-kemper-1",
    "title": "Sobre N√≥s",
    "section": "Davi Kemper",
    "text": "Davi Kemper\n\nCargo: Estudante de Ci√™ncia de Dados na FAE, atuou como Analista de BI do grupo Metronorte. üîó Portfolio"
  },
  {
    "objectID": "Projeto Finan√ßas/about.html#jo√£o-niquele",
    "href": "Projeto Finan√ßas/about.html#jo√£o-niquele",
    "title": "Sobre N√≥s",
    "section": "Jo√£o Niquele",
    "text": "Jo√£o Niquele\n\nCargo: Estudante de Ci√™ncia de Dados na FAE üîó Portfolio"
  },
  {
    "objectID": "FeatEngFinancial.html#introdu√ß√£o-feature-engineering-em-dados-de-s√©ries-financeiras",
    "href": "FeatEngFinancial.html#introdu√ß√£o-feature-engineering-em-dados-de-s√©ries-financeiras",
    "title": "Feature Engineering com s√©ries de pre√ßos de ativos financeiros",
    "section": "üìå Introdu√ß√£o: Feature Engineering em dados de s√©ries financeiras",
    "text": "üìå Introdu√ß√£o: Feature Engineering em dados de s√©ries financeiras\n\n\nMedindo a Volatilidade: Com e Sem GARCH\nA abordagem tradicional calcula a volatilidade como o desvio padr√£o dos retornos hist√≥ricos em uma janela m√≥vel de tamanho \\(N\\): (dias de negocia√ß√£o)\n\\[\n\\sigma_{\\text{hist}} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(r_i - \\bar{r})^2}\n\\]\nVantagens:\n\nSimplicidade e facilidade de implementa√ß√£o.\n\nDesvantagens:\n\nAssume volatilidade constante durante a janela. (ou seja precisa de um range dias e n√£o √© capaz de medir o desvio ou o risco/volatilidade de um dia pro outro.)\nN√£o capta a persist√™ncia dos choques (efeito de clustering).\nN√£o reage dinamicamente a choques recentes.\n\n\n\nVolatilidade com o GARCH\nO modelo GARCH(1,1) estima a vari√¢ncia condicional de forma din√¢mica:\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2.\n\\]\nOnde: - \\(\\epsilon_{t-1}^2\\) reflete o impacto dos choques recentes. - \\(\\sigma_{t-1}^2\\) reflete a persist√™ncia da volatilidade do per√≠odo anterior. - \\(\\omega &gt; 0\\), \\(\\alpha \\geq 0\\) e \\(\\beta \\geq 0\\) s√£o par√¢metros estimados.\nA soma \\(\\alpha + \\beta\\) mede a persist√™ncia total da volatilidade:\n\nPr√≥ximo de 1: Choques t√™m efeitos duradouros; a volatilidade permanece alta por v√°rios per√≠odos.\nMenor que 1: Os choques se dissipam mais rapidamente; a volatilidade retorna ao seu n√≠vel m√©dio mais r√°pido.\n\nVantagens do GARCH:\n\nModela a volatilidade de forma din√¢mica.\nCaptura o efeito de ‚Äúclustering‚Äù dos choques.\nPermite previs√µes mais precisas da volatilidade futura.\n\nDesvantagens do GARCH:\n\nRequer estima√ß√£o de par√¢metros e pressup√µe uma estrutura espec√≠fica para a volatilidade.\nPode ser sens√≠vel √† escolha da distribui√ß√£o dos res√≠duos (por exemplo, normal vs.¬†\\(t\\) de Student).\n\nVolatilidade com desvio-padr√£oVolatilidade com GARCH(1,1)Plotando retorno x risco\n\n\nA volatilidade hist√≥rica pode ser medida como o desvio-padr√£o dos log-retornos calculado em uma janela m√≥vel de \\(N\\) dias de negocia√ß√£o.\nNeste exemplo, adotamos uma janela m√≥vel de 5 dias (aproximadamente uma semana de negocia√ß√£o. Para um m√™s de negocia√ß√£o, utilizar 22 e um ano 252) para capturar a volatilidade di√°ria dos ativos.\nVantagens:\n\nSimplicidade e facilidade de implementa√ß√£o.\n\nDesvantagens:\n\nAssume volatilidade constante durante a janela.\nN√£o capta a persist√™ncia dos choques.\nN√£o reage dinamicamente a choques recentes.\n\n\n\nCode\n# Calcular a volatilidade hist√≥rica com uma janela m√≥vel de 5 dias\n\n# Supondo que 'log_returns' j√° foi calculado e possui a coluna 'date' e os log-retornos dos ativos\nwindow = 5\n\n# Cria um DataFrame para armazenar a volatilidade hist√≥rica\nvol_hist = pd.DataFrame({'date': log_returns[\"date\"]})\n\n# Calcula o desvio-padr√£o m√≥vel (volatilidade) para cada ativo\nfor col in log_returns.columns[1:]:\n    vol_hist[col] = log_returns[col].rolling(window=window).std()\n\n# Exibe as ultimas linhas do DataFrame de volatilidade hist√≥rica\n#print(vol_hist.head()) # 5 primeiros ser√£o NaN\nprint(vol_hist.tail())\n\n\n            date  BEEF3.SA  BRFS3.SA       GIS       HRL  JBSS3.SA  MRFG3.SA  \\\n1239  2025-04-04  0.018061  0.017195  0.019901  0.019937  0.013475  0.017031   \n1240  2025-04-07  0.017915  0.017151  0.020406  0.020119  0.012847  0.017031   \n1241  2025-04-08  0.016138  0.007259  0.024098  0.021751  0.008219  0.004705   \n1242  2025-04-09  0.042623  0.038677  0.028590  0.024068  0.023526  0.025228   \n1243  2025-04-10  0.040566  0.038906  0.021658  0.016153  0.022935  0.023309   \n\n           TSN  \n1239  0.032352  \n1240  0.027654  \n1241  0.027724  \n1242  0.037064  \n1243  0.036158  \n\n\nPlotando temos:\n\n\nCode\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\n# Certificar que \"date\" √© datetime (j√° feito)\nvol_hist['date'] = pd.to_datetime(vol_hist['date'])\n\n# Transformar para formato longo\nvol_hist_long = vol_hist.melt(id_vars='date', var_name='Ativo', value_name='Volatilidade_Hist')\n\nfig = px.line(vol_hist_long, x='date', y='Volatilidade_Hist', color='Ativo',\n              title='Volatilidade Hist√≥rica (Desvio-Padr√£o) com janela de 5 dias')\nfig.show()\n\n\n\n\nO desvio-padr√£o assume que precisaremos de um range maior do que 2 pontos no tempo, o que limita nossa an√°lise pois a incompatibiliza, uma vez que precisaremos comparar os riscos di√°rios x retornos di√°rios (como feito anteriormente). Ou seja, n√£o podemos comparar retornos dia-a-dia x volatilidades (risco) de 5 em 5 dias p.¬†ex.\nOs modelos heteroced√°sticos (da fam√≠lia ARCH) estimam a vari√¢ncia condicional dos nossos dados, ou seja, em linguagem de finan√ßas, eles s√£o capazes de capturar as volatilidades ou risco dos retornos dos pre√ßos de ativos financeiros ponto a ponto no tempo, ou seja, dia a dia.\nO modelo GARCH(1,1) com distribui√ß√£o \\(t\\) assim√©trica n√£o est√° dispon√≠vel diretamente na maioria das bibliotecas Python. No entanto, podemos utilizar um GARCH(1,1) com uma distribui√ß√£o \\(t\\) padr√£o para estimar a vari√¢ncia condicional. O modelo √© representado por:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nOnde:\n\n\\(r_t\\) √© o log-retorno no tempo \\(t\\).\n\\(\\mu\\) √© a m√©dia dos retornos.\n\\(\\epsilon_t\\) √© o termo de erro, condicionado √†s informa√ß√µes passadas.\n\\(\\sigma_t^2\\) √© a vari√¢ncia condicional no tempo \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) s√£o os par√¢metros a serem estimados, com \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) segue uma distribui√ß√£o \\(t\\) de Student com \\(ŒΩ\\) graus de liberdade para capturar as caudas pesadas observadas em retornos financeiros.\n\nA soma \\(\\alpha + \\beta\\) √© frequentemente utilizada para medir a persist√™ncia da volatilidade: quanto mais pr√≥ximos de 1, maior a persist√™ncia dos choques na volatilidade.\nVamos estimar a vari√¢ncia condicional (\\(\\sigma^2_{t}\\) ) para cada ativo:\n\n\nCode\n# Estimar o modelo GARCH(1,1) e salvar vari√¢ncia condicional\nvar_condicional = pd.DataFrame({\"date\": log_returns[\"date\"]})\n\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n    var_condicional[col] = res.conditional_volatility ** 2\n\nvar_condicional.head()\n\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007186. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009502. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001738. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002101. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0004777. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0008154. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002983. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\n\n\n\n\n\n\n\ndate\nBEEF3.SA\nBRFS3.SA\nGIS\nHRL\nJBSS3.SA\nMRFG3.SA\nTSN\n\n\n\n\n1\n2020-04-14\n0.006489\n0.003739\n0.000189\n0.002342\n0.000879\n0.001129\n0.002966\n\n\n2\n2020-04-15\n0.005795\n0.003823\n0.000202\n0.002844\n0.000781\n0.000866\n0.002396\n\n\n3\n2020-04-16\n0.005688\n0.003715\n0.000179\n0.002033\n0.000779\n0.000740\n0.002656\n\n\n4\n2020-04-17\n0.007190\n0.003715\n0.000222\n0.002954\n0.000672\n0.000705\n0.002138\n\n\n5\n2020-04-20\n0.006506\n0.003712\n0.000190\n0.002054\n0.000642\n0.000673\n0.002299\n\n\n\n\n\n\n\nVamos avaliar os par√¢metros estimados do modelo:\n\n\nCode\n# Inferir sobre os par√¢metros do modelo GARCH(1,1) para cada ativo do portf√≥lio\n\nparams_list = []\n\n# Iterar sobre cada ativo (exceto a coluna 'date')\nfor col in log_returns.columns[1:]:\n    am = arch_model(log_returns[col], vol=\"Garch\", p=1, q=1, dist=\"t\")\n    res = am.fit(disp=\"off\")\n\n    par = res.params\n    alpha_val = par.get(\"alpha[1]\", None)\n    beta_val  = par.get(\"beta[1]\", None)\n    alpha_beta_sum = (alpha_val if alpha_val is not None else 0) + (beta_val if beta_val is not None else 0)\n\n    # Interpreta√ß√£o curta\n    if alpha_beta_sum &gt;= 0.9:\n        interp = f\"Alta persist√™ncia (Œ±+Œ≤ = {alpha_beta_sum:.4f}).\"\n    else:\n        interp = f\"Baixa/moderada persist√™ncia (Œ±+Œ≤ = {alpha_beta_sum:.4f}).\"\n\n    params_list.append({\n         \"Ativo\": col,\n         \"mu\": par.get(\"mu\", None),\n         \"omega\": par.get(\"omega\", None),\n         \"alpha\": alpha_val,\n         \"beta\": beta_val,\n         \"alpha+beta\": alpha_beta_sum,\n         \"nu\": par.get(\"nu\", None),\n         \"Interpretacao\": interp\n    })\n\ngarch_params = pd.DataFrame(params_list)\n\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0007186. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0009502. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0001738. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002101. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0004777. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0008154. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\nC:\\Users\\kuiav\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\nestimating the model parameters. The scale of y is 0.0002983. Parameter\nestimation work better when this value is between 1 and 1000. The recommended\nrescaling is 100 * y.\n\nThis warning can be disabled by either rescaling y before initializing the\nmodel or by setting rescale=False.\n\n\n\nA soma \\(\\alpha + \\beta\\) √© um indicador crucial na modelagem GARCH para avaliar a persist√™ncia da volatilidade. Em termos pr√°ticos, os par√¢metros \\(\\alpha\\) e \\(\\beta\\) t√™m fun√ß√µes distintas:\n\n\\(\\alpha\\): Representa o impacto dos choques recentes (a inova√ß√£o ou termo de erro \\(\\epsilon_{t-1}^2\\) na volatilidade atual. Um valor mais alto de \\(\\alpha\\) indica que choques recentes t√™m um efeito maior em aumentar a volatilidade.\n\\(\\beta\\): Captura a persist√™ncia da volatilidade ao longo do tempo, ou seja, o efeito da volatilidade passada (\\(\\sigma_{t-1}^2)\\) sobre a volatilidade presente. Valores maiores de \\(\\beta\\) sugerem que a volatilidade tende a se manter elevada por um per√≠odo mais longo.\n\nQuando somamos esses dois par√¢metros, ou seja, quando calculamos \\(\\alpha + \\beta\\), obtemos uma medida da persist√™ncia total da volatilidade:\n\nSe \\(\\alpha + \\beta\\) estiver pr√≥ximo de 1, isso indica que os choques que afetam a volatilidade t√™m efeitos de longa dura√ß√£o. Em outras palavras, um choque na volatilidade tem um impacto que se dissipa muito lentamente, mantendo a volatilidade elevada por v√°rios per√≠odos.\nSe \\(\\alpha + \\beta\\) for significativamente menor que 1, os efeitos dos choques s√£o de curta dura√ß√£o e a volatilidade retorna rapidamente ao seu n√≠vel m√©dio ap√≥s um impacto.\n\nEm alguns casos, quando \\(\\alpha + \\beta = 1\\), o modelo √© denominado IGARCH (Integrated GARCH), o que implica que os choques t√™m efeitos persistentes permanentemente, ou seja, a volatilidade n√£o reverte para um valor m√©dio fixo.\nEsta caracter√≠stica √© particularmente importante na an√°lise de s√©ries financeiras, pois a persist√™ncia alta da volatilidade pode implicar maior risco de mercado e desafios na previs√£o dos retornos futuros. Assim, a soma \\(\\alpha + \\beta\\) serve como uma medida de ‚Äúmem√≥ria‚Äù dos choques, indicando se a volatilidade reage de forma passageira ou duradoura a eventos inesperados.\nGraficamente temos:\n\n\nCode\n# Retire o comando #| eval: false pra conseguir executar essa celula dentro do Quarto\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BRFS3.SA']].copy()\nvol_zc = var_condicional[['date', 'BRFS3.SA']].copy()\n\n# Converter \"date\" para datetime, se necess√°rio\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Di√°rios - BRFS3.SA\", \"Volatilidade Condicional (GARCH) - BRFS3.SA\")\n)\n\n# Adicionar o gr√°fico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BRFS3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gr√°fico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BRFS3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BRFS3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()\n\n\nEm alguns casos, a vari√¢ncia condicional pode apresentar grandes oscila√ß√µes se houver outliers nos retornos ou problemas de converg√™ncia do modelo. Verifique:\n\nQualidade e limpeza dos dados\nResumo do ajuste (par√¢metros \\(\\alpha,\\beta\\) plaus√≠veis?)\nDistribui√ß√£o (\\(t\\) vs.¬†normal)\nModelos alternativos (EGARCH, GJR-GARCH, etc.)\n\n\n\nAqui iremos visualizar o comportamento do ativo ZC=F, futuros de milho:\n\n\nCode\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Para o ativo \"ZC=F\"\nreturns_zc = log_returns[['date', 'BRFS3.SA']].copy()\nvol_zc = var_condicional[['date', 'BRFS3.SA']].copy()\n\n# Converter \"date\" para datetime, se necess√°rio\nreturns_zc['date'] = pd.to_datetime(returns_zc['date'])\nvol_zc['date'] = pd.to_datetime(vol_zc['date'])\n\n# Criar figura com dois subplots compartilhando o eixo x\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    subplot_titles=(\"Retornos Di√°rios - BRFS3.SA\", \"Volatilidade Condicional (GARCH) - BRFS3.SA\")\n)\n\n# Adicionar o gr√°fico de retornos\nfig.add_trace(\n    go.Scatter(x=returns_zc['date'], y=returns_zc['BRFS3.SA'], mode='lines', name='Retornos'),\n    row=1, col=1\n)\n\n# Adicionar o gr√°fico de volatilidade condicional\nfig.add_trace(\n    go.Scatter(x=vol_zc['date'], y=vol_zc['BRFS3.SA'], mode='lines', name='Volatilidade'),\n    row=2, col=1\n)\n\nfig.update_layout(\n    height=600,\n    width=900,\n    title_text=\"Retorno vs. Volatilidade (GARCH) - BRFS3.SA\",\n    xaxis2_title=\"Data\",\n    yaxis1_title=\"Retorno\",\n    yaxis2_title=\"Volatilidade Condicional\"\n)\n\nfig.show()\n\n\nNotem como o GARCH(1,1) captura bem os picos/quebras e na volatilidade dos retornos."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre N√≥s",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJo√£o Niquele"
  },
  {
    "objectID": "about.html#sum√°rio",
    "href": "about.html#sum√°rio",
    "title": "Sobre N√≥s",
    "section": "",
    "text": "Daniel K Junior\nArthur Lauffer\nDavi Kemper\nJo√£o Niquele"
  },
  {
    "objectID": "about.html#arthur-lauffer",
    "href": "about.html#arthur-lauffer",
    "title": "Sobre N√≥s",
    "section": "Arthur Lauffer",
    "text": "Arthur Lauffer\n\nCargo: √â analista de BI e estudante de Ci√™ncia de Dados para Neg√≥cios na FAE Business School. Ele administra sua pr√≥pria empresa de BI, prestando servi√ßos para outras empresas, e tamb√©m gerencia uma empresa de SaaS focada em projetos de longo prazo. Com grande experi√™ncia em Power BI, ele desenvolve dashboards e modelos de dados para diversas √°reas, incluindo vendas, RH e faturamento. Al√©m disso, atua como administrador do Workspace do Google da sua empresa. No tempo livre, tem interesse em m√∫sica eletr√¥nica e est√° organizando a¬†festa¬†Synapse. üîó Portfolio"
  },
  {
    "objectID": "about.html#davi-kemper",
    "href": "about.html#davi-kemper",
    "title": "Sobre N√≥s",
    "section": "Daniel K Junior",
    "text": "Daniel K Junior\n\nCargo: Formado na Escola de Sargento das Armas no ano de 2021, decidiu fazer a transi√ß√£o de carreira para a √°rea de Dados j√° no √≠nicio da faculdade, concluindo a transi√ß√£o no final do ano de 2024, hoje atua como Analista de BI na EZ Chart.\nüîó Portfolio"
  },
  {
    "objectID": "about.html#davi-kemper-1",
    "href": "about.html#davi-kemper-1",
    "title": "Sobre N√≥s",
    "section": "Davi Kemper",
    "text": "Davi Kemper\n\nCargo: Estudante de Ci√™ncia de Dados na FAE, atuou como Analista de BI do grupo Metronorte. üîó Portfolio"
  },
  {
    "objectID": "about.html#jo√£o-niquele",
    "href": "about.html#jo√£o-niquele",
    "title": "Sobre N√≥s",
    "section": "Jo√£o Niquele",
    "text": "Jo√£o Niquele\n\nCargo: Estudante de Ci√™ncia de Dados na FAE üîó Portfolio"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projeto Finan√ßas",
    "section": "",
    "text": "Usaremos seguintes a√ß√µes da bolsa :\n\nBRFS3: A BRF √© uma empresa transnacional brasileira do ramo aliment√≠cio, fruto da fus√£o entre Sadia e Perdig√£o, duas das principais empresas de alimentos do Brasil.\nJBSS3: JBS √© uma empresa brasileira do setor de alimentos fundada em 1953 em Goi√°s. A companhia opera no processamento de carnes bovina, su√≠na, ovina, de frango, de peixe e plant-based, al√©m de atuar no processamento de couros\nBEEF3: Minerva Foods √© uma empresa brasileira de alimentos fundada em 1924 na cidade de Barretos. A companhia tem atua√ß√£o na comercializa√ß√£o de carne in natura, couros, derivados, e na exporta√ß√£o de gado vivo, al√©m de atuar no processamento de carnes.\nMRFG3: Marfrig Global Foods √© uma empresa brasileira de alimentos. Fundada no ano 2000, √© a segunda maior produtora de carne bovina do mundo e l√≠der na produ√ß√£o de hamb√∫rgueres.\nTSN: A Tyson Foods √© uma empresa multinacional americana fundada por John W. Tyson em 1931 e sediada em Springdale, Arkansas, que opera na ind√∫stria aliment√≠cia.\nHRL: A Hormel Foods Corporation √© uma empresa aliment√≠cia estadunidense com sede em Austin, Minnesota, conhecida pela fabrica√ß√£o do Spam. Em 24 de agosto de 2017, a empresa anunciou a compra da empresa brasileira Ceratti.\nGIS: General Mills √© uma multinacional americana produtora de alimentos classificada na Fortune 500 e uma das 10 maiores empresas de alimentos do mundo. √â sediada em Golden Valley, Minnesota, Minneapolis.\n\nUtilizamos a API Yahoo! Finance para conseguir os dados utilizados para as analises a seguir.\nAnalisando os dados em uma tabela:\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(cowplot) \nlibrary(lattice)\nlibrary(timetk)\nlibrary(quantmod)\nlibrary(DT) \n\n\nTICKERS &lt;- c(\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\n\nportfolioPrices &lt;- NULL\nfor ( Ticker in TICKERS )\n  portfolioPrices &lt;- cbind(\n    portfolioPrices, \n    getSymbols(\n      Ticker,\n      src = \"yahoo\",\n      from = \"2019-01-01\",\n      auto.assign = FALSE\n    )[,4]\n  )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"BRFS3\",\n  \"JBSS3\",\n  \"BEEF3\",\n  \"MRFG3\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\nCode\n# Visualizar com DT\ndatatable(tail(portfolioPrices), options = list(pageLength = 10, scrollX = TRUE)) \n\n\n\n\n\n\nE ent√£o a gente faz uma analise temporal dos dados, tendo o eixo X sendo a vari√°vel tempo, e o eixo Y sendo o pre√ßo:\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along(GIS)\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n  ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )"
  },
  {
    "objectID": "Projeto Finan√ßas/index.html",
    "href": "Projeto Finan√ßas/index.html",
    "title": "Projeto Finan√ßas",
    "section": "",
    "text": "Usaremos seguintes a√ß√µes da bolsa :\n\nBRFS3: A BRF √© uma empresa transnacional brasileira do ramo aliment√≠cio, fruto da fus√£o entre Sadia e Perdig√£o, duas das principais empresas de alimentos do Brasil.\nJBSS3: JBS √© uma empresa brasileira do setor de alimentos fundada em 1953 em Goi√°s. A companhia opera no processamento de carnes bovina, su√≠na, ovina, de frango, de peixe e plant-based, al√©m de atuar no processamento de couros\nBEEF3: Minerva Foods √© uma empresa brasileira de alimentos fundada em 1924 na cidade de Barretos. A companhia tem atua√ß√£o na comercializa√ß√£o de carne in natura, couros, derivados, e na exporta√ß√£o de gado vivo, al√©m de atuar no processamento de carnes.\nMRFG3: Marfrig Global Foods √© uma empresa brasileira de alimentos. Fundada no ano 2000, √© a segunda maior produtora de carne bovina do mundo e l√≠der na produ√ß√£o de hamb√∫rgueres.\nTSN: A Tyson Foods √© uma empresa multinacional americana fundada por John W. Tyson em 1931 e sediada em Springdale, Arkansas, que opera na ind√∫stria aliment√≠cia.\nHRL: A Hormel Foods Corporation √© uma empresa aliment√≠cia estadunidense com sede em Austin, Minnesota, conhecida pela fabrica√ß√£o do Spam. Em 24 de agosto de 2017, a empresa anunciou a compra da empresa brasileira Ceratti.\nGIS: General Mills √© uma multinacional americana produtora de alimentos classificada na Fortune 500 e uma das 10 maiores empresas de alimentos do mundo. √â sediada em Golden Valley, Minnesota, Minneapolis.\n\nUtilizamos a API Yahoo! Finance para conseguir os dados utilizados para as analises a seguir.\nAnalisando os dados em uma tabela:\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(cowplot) \nlibrary(lattice)\nlibrary(timetk)\nlibrary(quantmod)\nlibrary(DT) \n\n\nTICKERS &lt;- c(\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\n\nportfolioPrices &lt;- NULL\nfor ( Ticker in TICKERS )\n  portfolioPrices &lt;- cbind(\n    portfolioPrices, \n    getSymbols(\n      Ticker,\n      src = \"yahoo\",\n      from = \"2019-01-01\",\n      auto.assign = FALSE\n    )[,4]\n  )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"BRFS3\",\n  \"JBSS3\",\n  \"BEEF3\",\n  \"MRFG3\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\n\n\nCode\n# Visualizar com DT\ndatatable(tail(portfolioPrices), options = list(pageLength = 10, scrollX = TRUE)) \n\n\n\n\n\n\nE ent√£o a gente faz uma analise temporal dos dados, tendo o eixo X sendo a vari√°vel tempo, e o eixo Y sendo o pre√ßo:\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along(GIS)\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n  ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )"
  },
  {
    "objectID": "page2.html",
    "href": "page2.html",
    "title": "Ci√™ncia de Dados para Neg√≥cios: Big Data for Finance Project",
    "section": "",
    "text": "Resumo\n\n\n\n\nteste de futuro para as a√ß√µes\n\n\n\n\nIntro\nescrever\n\nR\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(purrr)\nlibrary(tidyquant)\nlibrary(tsibble)\nlibrary(prophet)\nlibrary(feasts)\nlibrary(fable)\nlibrary(fabletools)\nlibrary(lubridate)\nlibrary(tictoc)\n\n\nCarregamos os dados:\n\n\nCode\ntickers &lt;- c(\n         \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n\nEnt√£o baixo os dados via Yahoo!Finance:\n\n\nCode\nportfolioPrices &lt;- NULL\n  for ( Ticker in tickers )\n    portfolioPrices &lt;- cbind(\n      portfolioPrices, \n      quantmod::getSymbols.yahoo(\n        Ticker,\n        from = \"2019-01-01\",\n        auto.assign = FALSE\n      )[,4]\n    )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"BRFS3.SA\",\n  \"JBSS3.SA\",\n  \"BEEF3.SA\",\n  \"MRFG3.SA\",\n  \"TSN\",\n  \"HRL\",\n  \"GIS\"\n)\n\n# Visualizar com DT\n#DT::datatable(tail(portfolioPrices), options = list(pageLength = 10, scrollX = TRUE)) \n\n\nVisualizando os dados dos nossos √∫ltimos retornos dos pre√ßos, temos:\n\n\nCode\nlog_returns &lt;- log(portfolioPrices) - log(lag(portfolioPrices))\nlog_returns &lt;- na.omit(log_returns)\nlog_returns &lt;- log_returns |&gt; \n  timetk::tk_tbl(preserve_index = TRUE, rename_index = \"date\")\n\ntail(log_returns)\n\n\n\n  \n\n\n\n\n\nCode\nln_returns &lt;- log_returns\n\nln_returns |&gt; as.data.frame() |&gt;\n  dplyr::mutate(\n    time = seq_along( TSN )\n  ) |&gt; select(-date) |&gt;\n  tidyr::pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  dplyr::group_by(Variables) |&gt;\n  timetk::plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  ggplot2::theme(\n    strip.background = ggplot2::element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\n\nModelagem com fpp3 e valida√ß√£o cruzada temporal\nPrecisaremos fazer um forecasting de curto prazo com nossos dados hist√≥ricos de retornos pra formularmos nossas recomenda√ß√µes posteriores de compra, venda e espera:\n\nVamos come√ßar com uma s√©rie por vez \\(\\Rightarrow\\) TSN\n\n\n\nCode\n# Primeiro converto pra tsibble\n\nlnretTSN &lt;- log_returns |&gt; \n  select(date, TSN) |&gt; \n  as_tsibble(index = date)\n\nglimpse(lnretTSN)\n\n\nRows: 1,519\nColumns: 2\n$ date &lt;date&gt; 2019-01-03, 2019-01-04, 2019-01-07, 2019-01-08, 2019-01-09, 2019‚Ä¶\n$ TSN  &lt;dbl&gt; 0.0211432803, 0.0122208208, 0.0160060857, 0.0264099860, -0.017528‚Ä¶\n\n\n\n\nCode\ntreino &lt;- lnretTSN |&gt;\n  filter_index(~\"2025-01-01\")\n\n\nWar models\n\n\nCode\ntic()\n\nModelos &lt;- treino |&gt;\n  model(\n    AjusteExp = ETS(TSN ~ error(\"A\") + trend(\"N\") + season(\"N\")), # Ajuste Exponencial com auto\n    \n    AjExp_aditivo = ETS(TSN ~ error(\"A\") + trend(\"A\") + season(\"A\")), # Ajuste Exponencial Aditivo\n    \n    AjExp_multiplicativo = ETS(TSN ~ error(\"M\") + trend(\"A\") + season(\"M\")), # Ajuste Exponencial Multiplicativo\n    \n    Croston = CROSTON(TSN), # Modelo Croston\n    \n    HoltWinters = ETS(TSN ~ error(\"M\") + trend(\"Ad\") + season(\"M\")), # Holt Winters\n    \n    Holt = ETS(TSN ~ error(\"A\") + trend(\"A\") + season(\"N\")), # Holt\n    \n    HoltAmort = ETS(TSN ~ error(\"A\") + trend(\"Ad\", phi = 0.9) + season(\"N\")), # Holt Amortecida\n    \n    Regr_Comp = TSLM(TSN ~ trend() + season()), # Regressao com tendencia e sazonalidade auto\n    \n    Regr_Harmonica = TSLM(TSN ~ trend() + fourier(K = 2)), # Regressao harmonica\n    \n    Regr_Quebras = TSLM(TSN ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n    \n    Snaive = SNAIVE(TSN), # SNAIVE\n    \n    Naive = NAIVE(TSN), #NAIVE\n    \n    Media_Movel = ARIMA(TSN ~ pdq(0,0,1)), # Media Movel Simples\n    \n    autoARIMA = ARIMA(TSN, stepwise = FALSE, approx = FALSE), # Auto ARIMA\n    \n    autoARIMA_saz = ARIMA(TSN, stepwise = FALSE, approx = FALSE, seasonal = TRUE), # AutoARIMA Sazonal\n    \n    #    Regr_erros_ARIMA = auto.arima(TSN, xreg = fourier(K = 3), seasonal = FALSE), # Regressao com erros ARIMA\n    \n    ARIMA_saz_012011 = ARIMA(TSN ~ pdq(0,1,2) + PDQ(0,1,1)), # ARIMA Sazonal ordem 012011\n    \n    ARIMA_saz_210011 = ARIMA(TSN ~ pdq(2,1,0) + PDQ(0,1,1)), # ARIMA Sazonal ordem 210011\n    \n    ARIMA_saz_0301012 = ARIMA(TSN ~ 0 + pdq(3,0,1) + PDQ(0,1,2)), # ARIMA sazonal\n    \n    ARIMA_quad = ARIMA(TSN ~ I(trend()^2)), # ARIMA com tendencia temporal quadratica\n    \n    ARIMA_determ = ARIMA(TSN ~ 1 + trend() + pdq(d = 0)), # ARIMA com tendencia deterministica\n    \n    ARIMA_estocastico = ARIMA(TSN ~ pdq(d = 1)), # ARIMA com tend√™ncia estocastica\n    \n    Regr_Harm_dinamica = ARIMA(TSN ~ fourier(K=2) + PDQ(0,0,0)), # Regressao Harmonica Dinamica\n    \n    Regr_Harm_Din_MultSaz = ARIMA(TSN ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = 7*30, K = 10) + fourier(period = 7*30, K = 5)), \n    \n    Regr_Harm_Din_Saz = ARIMA(TSN ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = \"month\", K = 10) +\n                                fourier(period = \"year\", K = 2) ), # Rgr Harm Mult Saz Complexa\n    \n#    Auto_Prophet = prophet(TSN), # Auto prophet\n    \n#    Prophet_mult = prophet(TSN ~ season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_aditivo = prophet(TSN ~ season(period = \"month\", order = 2, type = \"additive\")),\n    \n#    Prophet_geom = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_memo = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 5) +\n#                             season(period = \"year\", order = 2, type = \"multiplicative\")),\n    \n    Modelo_VAR = VAR(TSN, ic = \"bic\"), # Vetor Autoregressivo \n    \n    Random_Walk = RW(TSN ~ drift()), # Random Walk com drift\n    \n    Rede_Neural_AR = NNETAR(TSN, bootstrap =  TRUE)#, # Rede Neural com auto AR e bootstraping nos erros\n    \n    #    x11 = X_13ARIMA_SEATS(TSN ~ x11()) # X11 ARIMA Seats\n    \n  ) |&gt;\n  \n  forecast(h = \"24 months\") # Horizonte de projecao para os proximos 30 dias apos corte no treino\n\ntoc()  \n\n\n0.93 sec elapsed\n\n\nSelecionamos o melhor modelo (1 fold de valida√ß√£o cruzada somente):\n\n\nCode\nModelos |&gt;\n  accuracy(lnretTSN) |&gt;\n  arrange(RMSE) # Sele√ß√£o da acuracia pelo menor RMSE para o conjunto de modelos\n\n\n\n  \n\n\n\nGero um cen√°rio com o modelo:\n\n\nCode\nfit &lt;- lnretTSN |&gt;\n  model(\n    Regr_Quebras = TSLM(TSN ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n  )\n\nsim &lt;- fit |&gt; generate(h = 30, times = 5, bootstrap = TRUE)\n\n\nPlotamos os forecasts com esse modelo pra tr√™s cen√°rios distintos no futuro:\n\n\nCode\nlnretTSN |&gt;\n  filter_index(\"2025-01-01\"~.) |&gt;\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = TSN)) +\n  geom_line(aes(y = .sim, colour = as.factor(.rep)),\n    data = sim) +\n  labs(title=\"Valores projetados de retornos de pre√ßos de contratos futuros da TSN\", y=\"$US\" ) +\n  guides(colour = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Primeiro converto pra tsibble\n\nlnretGIS &lt;- log_returns |&gt; \n  select(date, GIS) |&gt; \n  as_tsibble(index = date)\n\nglimpse(lnretGIS)\n\n\nRows: 1,519\nColumns: 2\n$ date &lt;date&gt; 2019-01-03, 2019-01-04, 2019-01-07, 2019-01-08, 2019-01-09, 2019‚Ä¶\n$ GIS  &lt;dbl&gt; 0.0154921374, 0.0200387411, 0.0164387227, 0.0149567729, -0.016440‚Ä¶\n\n\n\n\nCode\ntreino &lt;- lnretGIS |&gt;\n  filter_index(~\"2025-01-01\")\n\n\n\n\nCode\ntic()\n\nModelos &lt;- treino |&gt;\n  model(\n    AjusteExp = ETS(GIS ~ error(\"A\") + trend(\"N\") + season(\"N\")), # Ajuste Exponencial com auto\n    \n    AjExp_aditivo = ETS(GIS ~ error(\"A\") + trend(\"A\") + season(\"A\")), # Ajuste Exponencial Aditivo\n    \n    AjExp_multiplicativo = ETS(GIS ~ error(\"M\") + trend(\"A\") + season(\"M\")), # Ajuste Exponencial Multiplicativo\n    \n    Croston = CROSTON(GIS), # Modelo Croston\n    \n    HoltWinters = ETS(GIS ~ error(\"M\") + trend(\"Ad\") + season(\"M\")), # Holt Winters\n    \n    Holt = ETS(GIS ~ error(\"A\") + trend(\"A\") + season(\"N\")), # Holt\n    \n    HoltAmort = ETS(GIS ~ error(\"A\") + trend(\"Ad\", phi = 0.9) + season(\"N\")), # Holt Amortecida\n    \n    Regr_Comp = TSLM(GIS ~ trend() + season()), # Regressao com tendencia e sazonalidade auto\n    \n    Regr_Harmonica = TSLM(GIS ~ trend() + fourier(K = 2)), # Regressao harmonica\n    \n    Regr_Quebras = TSLM(GIS ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n    \n    Snaive = SNAIVE(GIS), # SNAIVE\n    \n    Naive = NAIVE(GIS), #NAIVE\n    \n    Media_Movel = ARIMA(GIS ~ pdq(0,0,1)), # Media Movel Simples\n    \n    autoARIMA = ARIMA(GIS, stepwise = FALSE, approx = FALSE), # Auto ARIMA\n    \n    autoARIMA_saz = ARIMA(GIS, stepwise = FALSE, approx = FALSE, seasonal = TRUE), # AutoARIMA Sazonal\n    \n    #    Regr_erros_ARIMA = auto.arima(TSN, xreg = fourier(K = 3), seasonal = FALSE), # Regressao com erros ARIMA\n    \n    ARIMA_saz_012011 = ARIMA(GIS ~ pdq(0,1,2) + PDQ(0,1,1)), # ARIMA Sazonal ordem 012011\n    \n    ARIMA_saz_210011 = ARIMA(GIS ~ pdq(2,1,0) + PDQ(0,1,1)), # ARIMA Sazonal ordem 210011\n    \n    ARIMA_saz_0301012 = ARIMA(GIS ~ 0 + pdq(3,0,1) + PDQ(0,1,2)), # ARIMA sazonal\n    \n    ARIMA_quad = ARIMA(GIS ~ I(trend()^2)), # ARIMA com tendencia temporal quadratica\n    \n    ARIMA_determ = ARIMA(GIS ~ 1 + trend() + pdq(d = 0)), # ARIMA com tendencia deterministica\n    \n    ARIMA_estocastico = ARIMA(GIS ~ pdq(d = 1)), # ARIMA com tend√™ncia estocastica\n    \n    Regr_Harm_dinamica = ARIMA(GIS ~ fourier(K=2) + PDQ(0,0,0)), # Regressao Harmonica Dinamica\n    \n    Regr_Harm_Din_MultSaz = ARIMA(GIS ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = 7*30, K = 10) + fourier(period = 7*30, K = 5)), \n    \n    Regr_Harm_Din_Saz = ARIMA(GIS ~ PDQ(0, 0, 0) + pdq(d = 0) + fourier(period = \"month\", K = 10) +\n                                fourier(period = \"year\", K = 2) ), # Rgr Harm Mult Saz Complexa\n    \n#    Auto_Prophet = prophet(TSN), # Auto prophet\n    \n#    Prophet_mult = prophet(TSN ~ season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_aditivo = prophet(TSN ~ season(period = \"month\", order = 2, type = \"additive\")),\n    \n#    Prophet_geom = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 2, type = \"multiplicative\")),\n    \n#    Prophet_memo = prophet(TSN ~ growth(\"geometric\") + season(period = \"month\", order = 5) +\n#                             season(period = \"year\", order = 2, type = \"multiplicative\")),\n    \n    Modelo_VAR = VAR(GIS, ic = \"bic\"), # Vetor Autoregressivo \n    \n    Random_Walk = RW(GIS ~ drift()), # Random Walk com drift\n    \n    Rede_Neural_AR = NNETAR(GIS, bootstrap =  TRUE)#, # Rede Neural com auto AR e bootstraping nos erros\n    \n    #    x11 = X_13ARIMA_SEATS(TSN ~ x11()) # X11 ARIMA Seats\n    \n  ) |&gt;\n  \n  forecast(h = \"24 months\") # Horizonte de projecao para os proximos 30 dias apos corte no treino\n\ntoc()  \n\n\n0.83 sec elapsed\n\n\n\n\nCode\nfit &lt;- lnretGIS |&gt;\n  model(\n    Regr_Quebras = TSLM(GIS ~ trend(knots = c(2018, 2019, 2020))), # Regressao com quebras estruturais\n  )\n\nsim &lt;- fit |&gt; generate(h = 30, times = 5, bootstrap = TRUE)\n\n\n\n\nCode\nlnretTSN |&gt;\n  filter_index(\"2025-01-01\"~.) |&gt;\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = TSN)) +\n  geom_line(aes(y = .sim, colour = as.factor(.rep)),\n    data = sim) +\n  labs(title=\"Valores projetados de retornos de pre√ßos de contratos futuros da SALESFORCE\", y=\"$US\" ) +\n  guides(colour = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\n¬†\n\n\n\nReferences\n\nMarkowitz, H. (1952). Portfolio Selection. The Journal of Finance, 7(1), 77‚Äì91.\nLink\nSharpe, W. F. (1966). Mutual Fund Performance. The Journal of Business, 39(1), 119‚Äì138.\nLink\nElton, E. J., Gruber, M. J., Brown, S. J., & Goetzmann, W. N. (2007). Modern Portfolio Theory and Investment Analysis (9th ed.). Wiley.\nHilpisch, Y. (2018). Python for Finance: Mastering Data-Driven Finance. O‚ÄôReilly Media."
  }
]